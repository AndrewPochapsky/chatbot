{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewPochapsky/chatbot/blob/master/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0I5A7ULV5Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from ast import literal_eval\n",
        "from fastai.layers import CrossEntropyFlat\n",
        "import spacy\n",
        "import pickle\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS74TvsTbjQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = Path('drive/My Drive/datasets/cornell movie-dialogs corpus')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRhpvb63aYdk",
        "colab_type": "code",
        "outputId": "09473eda-0e9a-4e0f-fc82-ad6335e03976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc78yc7jY3To",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTVhRWBJkSbX",
        "colab_type": "text"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthfvrbqY7hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(s):\n",
        "    s = s.replace('\\n',' ').lower()\n",
        "    return s\n",
        "\n",
        "def tokenize(corpus, vocab = None):\n",
        "    tokenizer = spacy.blank(\"en\").tokenizer\n",
        "    doc = tokenizer(corpus)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if(token.text.strip() != \"\"):\n",
        "            if(vocab != None and token.text not in vocab):\n",
        "                tokens.append('xxxunk')\n",
        "            else:\n",
        "                tokens.append(token.text)\n",
        "    return tokens\n",
        "\n",
        "def process_dataset():\n",
        "    all_words = \"\"\n",
        "    with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            all_words += parts[-1]\n",
        "    return all_words\n",
        "\n",
        "def generate_vocab(tokens, min_freq = 0):\n",
        "    all_unique_words_counter = Counter(tokens)\n",
        "    vocab = {}\n",
        "    vocab['xxxpad'] = 0\n",
        "    vocab['xxxeos'] = 1\n",
        "    vocab['xxxbos'] = 2\n",
        "    vocab['xxxunk'] = 3\n",
        "    index = 4\n",
        "    for w in all_unique_words_counter.keys():\n",
        "        if(all_unique_words_counter[w] >= min_freq and w.strip() != \"\"):\n",
        "            vocab[w] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "def replace_with_unk(tokens, vocab):\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i] if tokens[i] in vocab else 'xxxunk'\n",
        "        \n",
        "\n",
        "def subsample(tokens, t = 1e-5):\n",
        "    \"\"\"\n",
        "        Paper: https://arxiv.org/pdf/1310.4546.pdf\n",
        "    \"\"\"\n",
        "    sampled_tokens = []\n",
        "    counter = Counter(tokens)\n",
        "    for token in tokens:\n",
        "        f_w = counter[token]/len(tokens)\n",
        "        p_w = 1 - math.sqrt(t/f_w)\n",
        "        val = random.uniform(0, 1)\n",
        "        if(val >= p_w):\n",
        "            sampled_tokens.append(token)\n",
        "            \n",
        "    return sampled_tokens\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "def create_training_matrices(vocab, all_words, window_size = 5):\t\n",
        "\t\"\"\"\n",
        "        Returns x_train: Tensor()\n",
        "    \"\"\"\n",
        "\tnumTotalWords = len(all_words)\n",
        "\txTrain=[]\n",
        "\tyTrain=[]\n",
        "\tfor i in range(numTotalWords):\n",
        "\t\twordsAfter = all_words[i + 1:i + window_size + 1]\n",
        "\t\twordsBefore = all_words[max(0, i - window_size):i]\n",
        "\t\twordsAdded = wordsAfter + wordsBefore\n",
        "\t\tfor word in wordsAdded:\n",
        "\t\t\txTrain.append(vocab[all_words[i]])\n",
        "\t\t\tyTrain.append(vocab[word])\n",
        "\treturn Tensor(xTrain), Tensor(yTrain)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUKWTcYDu9_j",
        "colab_type": "code",
        "outputId": "e417ca11-8604-4071-bca8-49cdc3b31f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "\n",
        "full_corpus = process_dataset() \n",
        "full_corpus = preprocess(full_corpus)\n",
        "print('Begin Tokenization')\n",
        "tokens = tokenize(full_corpus) # list of string\n",
        "print('Generating vocab')\n",
        "vocab = generate_vocab(tokens, min_freq = 2)\n",
        "\n",
        "replace_with_unk(tokens, vocab)\n",
        "\n",
        "print(len(tokens))\n",
        "print('Subsampling data')\n",
        "tokens = subsample(tokens, 1e-5)\n",
        "print(len(tokens))\n",
        "print('Getting training data')\n",
        "x_train, y_train = create_training_matrices(vocab, tokens, window_size = 3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Tokenization\n",
            "Generating vocab\n",
            "4194111\n",
            "Subsampling data\n",
            "666195\n",
            "Getting training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94YV0Yi-2rnJ",
        "colab_type": "text"
      },
      "source": [
        "SkipGram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avoAJTVmuGPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, emb_size, emb_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.center_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.context_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.init_emb()\n",
        "    \n",
        "    def init_emb(self):\n",
        "        #initrange = 0.5 / self.emb_dim\n",
        "        #self.center_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        nn.init.kaiming_uniform_(self.center_embeddings.weight, a=math.sqrt(5))\n",
        "        self.context_embeddings.weight.data.uniform_(-0, 0)\n",
        "        \n",
        "    def forward(self, pos_center, pos_context, neg_context):\n",
        "        losses = []\n",
        "        emb_center = self.center_embeddings(pos_center.long())\n",
        "        emb_context = self.context_embeddings(pos_context.long())\n",
        "        score = torch.mul(emb_center, emb_context).squeeze()\n",
        "        #print(score.shape)\n",
        "        score = torch.sum(score, dim = 1)\n",
        "        score = F.logsigmoid(score) # I think it is logsigmoid since we are doing nll loss func?\n",
        "        losses.append(sum(score))\n",
        "        \n",
        "        \n",
        "        neg_emb_context = self.context_embeddings(neg_context.long())\n",
        "        #print(neg_emb_context.shape)\n",
        "        neg_score = torch.bmm(neg_emb_context, emb_center.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.sum(neg_score, dim = 1)\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        losses.append(sum(neg_score))\n",
        "        return -1 * sum(losses)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QZ94UCm_pfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size = 128, shuffle = True, num_workers = 4)\n",
        "\n",
        "def map_to_index(np_array, vocab):\n",
        "    output = torch.zeros(np_array.shape)\n",
        "    for i in range(len(np_array)):\n",
        "        output[i] = Tensor(list(map(lambda x: vocab[x], np_array[i])))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_JMxP2N2-al",
        "colab_type": "code",
        "outputId": "478d3a7d-ca3c-4eb3-fc05-6b4365efdc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "lr = 3e-3\n",
        "\n",
        "num_epochs = 10\n",
        "neg_sample_size = 5\n",
        "emb_dim = 100\n",
        "emb_size = len(vocab.keys())\n",
        "model = SkipGramModel(emb_size, emb_dim)\n",
        "model.load_state_dict(torch.load(base_path/'word2vec0.pt'))\n",
        "optim = torch.optim.SGD(model.parameters(), lr = lr) #cant use mom or wd since that would require calculating for all the params, too expensive\n",
        "i = 0\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    i = 0\n",
        "    for xb, yb in train_dl:\n",
        "        #neg sampling\n",
        "        neg_context = np.random.choice(\n",
        "            tokens,\n",
        "            size=(len(xb), neg_sample_size)\n",
        "        )\n",
        "        \n",
        "        neg_context = map_to_index(neg_context, vocab)\n",
        "        #print(type(neg_context))\n",
        "        optim.zero_grad()\n",
        "        loss = model(xb, yb, neg_context)\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if(i % 100 == 0):\n",
        "            print('completed {0}/{1} batches. Avg loss per batch: {2}'.format(i, len(train_dl), total_loss/(i)))\n",
        "        i+=1\n",
        "    \n",
        "    torch.save(model.state_dict(), base_path/'word2vec{0}.pt'.format(epoch))\n",
        "    \n",
        "    \n",
        "torch.save(model.state_dict(), base_path/'word2vec.pt')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed 0/31234 batches. Avg loss per batch: inf\n",
            "completed 100/31234 batches. Avg loss per batch: 153.0912322998047\n",
            "completed 200/31234 batches. Avg loss per batch: 152.4522247314453\n",
            "completed 300/31234 batches. Avg loss per batch: 152.2268524169922\n",
            "completed 400/31234 batches. Avg loss per batch: 152.18731689453125\n",
            "completed 500/31234 batches. Avg loss per batch: 152.10638427734375\n",
            "completed 600/31234 batches. Avg loss per batch: 152.03564453125\n",
            "completed 700/31234 batches. Avg loss per batch: 151.99029541015625\n",
            "completed 800/31234 batches. Avg loss per batch: 151.93702697753906\n",
            "completed 900/31234 batches. Avg loss per batch: 151.91429138183594\n",
            "completed 1000/31234 batches. Avg loss per batch: 151.90428161621094\n",
            "completed 1100/31234 batches. Avg loss per batch: 151.91932678222656\n",
            "completed 1200/31234 batches. Avg loss per batch: 151.88522338867188\n",
            "completed 1300/31234 batches. Avg loss per batch: 151.81710815429688\n",
            "completed 1400/31234 batches. Avg loss per batch: 151.77047729492188\n",
            "completed 1500/31234 batches. Avg loss per batch: 151.72305297851562\n",
            "completed 1600/31234 batches. Avg loss per batch: 151.7111053466797\n",
            "completed 1700/31234 batches. Avg loss per batch: 151.68276977539062\n",
            "completed 1800/31234 batches. Avg loss per batch: 151.62599182128906\n",
            "completed 1900/31234 batches. Avg loss per batch: 151.58807373046875\n",
            "completed 2000/31234 batches. Avg loss per batch: 151.54115295410156\n",
            "completed 2100/31234 batches. Avg loss per batch: 151.49571228027344\n",
            "completed 2200/31234 batches. Avg loss per batch: 151.45960998535156\n",
            "completed 2300/31234 batches. Avg loss per batch: 151.42123413085938\n",
            "completed 2400/31234 batches. Avg loss per batch: 151.3830108642578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-137a60546c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFZBp1wWtt8U",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcMUbFbcGTea",
        "colab_type": "text"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukHUcLxPGQdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line_map = {}\n",
        "with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        line_num = parts[0]\n",
        "        #-2 to get rid of \\n\n",
        "        text = parts[-1][:-1]\n",
        "        line_map[line_num] = text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0opgwHBGRGH",
        "colab_type": "code",
        "outputId": "d81e72b3-5dcc-42fa-c8cb-7df64f08abab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "table = []\n",
        "with open(base_path/'movie_conversations.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        #get the referenced line numbers\n",
        "        line_nums = re.findall('L[0-9]+', parts[-1])\n",
        "        #form pairs\n",
        "        \n",
        "        for i in range(len(line_nums) - 1):\n",
        "            pair = (line_nums[i], line_nums[i+1])\n",
        "            #df.loc[df['column_name'] == some_value]\n",
        "            first = line_map[line_nums[i]]\n",
        "            second = line_map[line_nums[i+1]]\n",
        "            table.append([tokenize(preprocess(first), vocab = vocab), tokenize(preprocess(second), vocab = vocab)])\n",
        "        \n",
        "data_df = pd.DataFrame(table, columns = ['in', 'out'])\n",
        "data_df.to_csv(base_path/'processed_data.csv', index = False)\n",
        "data_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>in</th>\n",
              "      <th>out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[can, we, make, this, quick, ?, xxxunk, xxxunk...</td>\n",
              "      <td>[well, ,, i, thought, we, 'd, start, with, xxx...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[well, ,, i, thought, we, 'd, start, with, xxx...</td>\n",
              "      <td>[not, the, hacking, and, gagging, and, spittin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[not, the, hacking, and, gagging, and, spittin...</td>\n",
              "      <td>[okay, ..., then, how, 'bout, we, try, out, so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[you, 're, asking, me, out, ., that, 's, so, c...</td>\n",
              "      <td>[forget, it, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[no, ,, no, ,, it, 's, my, fault, --, we, did,...</td>\n",
              "      <td>[cameron, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  in                                                out\n",
              "0  [can, we, make, this, quick, ?, xxxunk, xxxunk...  [well, ,, i, thought, we, 'd, start, with, xxx...\n",
              "1  [well, ,, i, thought, we, 'd, start, with, xxx...  [not, the, hacking, and, gagging, and, spittin...\n",
              "2  [not, the, hacking, and, gagging, and, spittin...  [okay, ..., then, how, 'bout, we, try, out, so...\n",
              "3  [you, 're, asking, me, out, ., that, 's, so, c...                                    [forget, it, .]\n",
              "4  [no, ,, no, ,, it, 's, my, fault, --, we, did,...                                       [cameron, .]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFiis_fjY2b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def converter(x):\n",
        "    #convert \"list\" to list\n",
        "    return literal_eval(x)\n",
        "\n",
        "converters={'in': converter, 'out': converter}\n",
        "df = pd.read_csv(base_path/'processed_data.csv', converters = converters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Dj7BZNGYdw",
        "colab_type": "text"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ocnidotyGS",
        "colab_type": "code",
        "outputId": "f5990664-804f-429b-d1ff-aa5209b6f5f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "emb_dim = 100\n",
        "emb_size = len(vocab.keys())\n",
        "word2vec_model = SkipGramModel(emb_size, emb_dim)\n",
        "word2vec_model.load_state_dict(torch.load(base_path/'word2vec0.pt'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ifkijpHmkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec7da498-82be-45e2-df5c-46242971c360"
      },
      "source": [
        "np.percentile([len(row['in']) for index, row in df.iterrows()], 90)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RMlUMGdEnAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b3d47d2-282a-4873-93bf-90ebe3199729"
      },
      "source": [
        "np.percentile([len(row['out']) for index, row in df.iterrows()], 90)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hE7FEaYIMBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(word2vec_model, emb_size, emb_dim, padding_idx, eos_idx):\n",
        "    new_emb = nn.Embedding(emb_size, emb_dim, padding_idx=padding_idx)\n",
        "    old_emb_weights = word2vec_model.center_embeddings.weight.data\n",
        "    for i in range(len(old_emb_weights)):\n",
        "        new_emb.weight.data[i] = old_emb_weights[i]\n",
        "    \n",
        "    #init the new embeddings to zero\n",
        "    new_emb.weight.data[padding_idx].uniform_(-old_emb_weights.mean(), old_emb_weights.mean())\n",
        "    new_emb.weight.data[eos_idx].uniform_(-old_emb_weights.mean(), old_emb_weights.mean())\n",
        "    \n",
        "    return new_emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v2JG88euzjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_emb = create_embedding_matrix(word2vec_model, emb_size, emb_dim, vocab['xxxpad'], vocab['xxxeos'])    \n",
        "dec_emb = create_embedding_matrix(word2vec_model, emb_size, emb_dim, vocab['xxxpad'], vocab['xxxeos'])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106s1eIDb2rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_x_and_y(df):\n",
        "    max_x = -1\n",
        "    for x in df['in']:\n",
        "        if(len(x) > max_x):\n",
        "            max_x = len(x)\n",
        "            \n",
        "    max_y = -1\n",
        "    for y in df['out']:\n",
        "        if(len(y) > max_y):\n",
        "            max_y = len(y)\n",
        "    max_y += 1\n",
        "    with open(base_path/'max.txt', 'w+') as f:\n",
        "        f.write(str(max_x) + ',' + str(max_y))\n",
        "    \n",
        "    return max_x, max_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSme7GyGcKnj",
        "colab_type": "code",
        "outputId": "d34a909d-1935-42e5-e04a-fae7d60a584b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_max_x_and_y(df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(370, 683)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO1om97EcNQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_x = None\n",
        "max_y = None\n",
        "with open(base_path/'max.txt') as f:\n",
        "    for line in f:\n",
        "        max_x, max_y = line.split(',')\n",
        "        max_x = int(max_x)\n",
        "        max_y = int(max_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfoUy4ZNwXV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_x_y_tensors(df, vocab, max_x = 30, max_y = 30):\n",
        "    num_rows = len(df.index)\n",
        "    num_valid_samples = 0\n",
        "    for index, row in df.iterrows():\n",
        "        #do < instead of <= so there is room for a token if I choose to add\n",
        "        if(len(row['in']) < max_x and len(row['out']) < max_y):\n",
        "            num_valid_samples += 1\n",
        "    res_x = torch.zeros(num_valid_samples, max_x).long() \n",
        "    res_y = torch.zeros(num_valid_samples, max_y).long()\n",
        "    tensor_idx = 0\n",
        "    for row_idx in range(num_rows):\n",
        "        if(row_idx % 10000 == 0): print('done {0}/{1} samples'.format(row_idx, num_rows))\n",
        "        \n",
        "        x, y = df.iloc[row_idx, :]\n",
        "        \n",
        "        if(len(x) >= max_x or len(y) >= max_y): continue\n",
        "        \n",
        "        x_tensor = torch.zeros(max_x) + vocab['xxxpad']\n",
        "        y_tensor = torch.zeros(max_y) + vocab['xxxpad']\n",
        "\n",
        "        num_padding = max_x - len(x)\n",
        "         \n",
        "        #populate the rest of it with actual input\n",
        "        token_index = 0;\n",
        "        for i in range(num_padding, max_x):\n",
        "            x_tensor[i] = vocab[x[token_index]] \n",
        "            token_index += 1\n",
        "               \n",
        "            \n",
        "        #add input to the output\n",
        "        for i in range(len(y)):\n",
        "            y_tensor[i] = vocab[y[i]]\n",
        "            \n",
        "        #add end of stream token\n",
        "        y_tensor[len(y)] = vocab['xxxeos']\n",
        "            \n",
        "        res_x[tensor_idx] = x_tensor.long()\n",
        "        res_y[tensor_idx] = y_tensor.long()\n",
        "        tensor_idx += 1\n",
        "        \n",
        "    return res_x, res_y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUHOYTLaun0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x and y obtained from get_x_y_tensors()\n",
        "def split_train_valid(all_x, all_y, valid_pct = 0.2):\n",
        "    num_x_samples, x_len = all_x.shape\n",
        "    num_y_samples, y_len = all_y.shape\n",
        "    \n",
        "    assert num_x_samples == num_y_samples\n",
        "    \n",
        "    x_train = torch.zeros(num_x_samples, x_len)\n",
        "    y_train = torch.zeros(num_y_samples, y_len)\n",
        "    \n",
        "    x_valid = torch.zeros(num_x_samples, x_len)\n",
        "    y_valid = torch.zeros(num_y_samples, y_len)\n",
        "    \n",
        "    train_idx = 0\n",
        "    valid_idx = 0\n",
        "    \n",
        "    for x, y in zip(all_x, all_y):\n",
        "        rand_num = random.uniform(0, 1)\n",
        "        if(rand_num >= 0.2):\n",
        "            x_train[train_idx] = x.squeeze(0)\n",
        "            y_train[train_idx] = y.squeeze(0)\n",
        "            train_idx += 1\n",
        "        else:\n",
        "            x_valid[valid_idx] = x.squeeze(0)\n",
        "            y_valid[valid_idx] = y.squeeze(0)\n",
        "            valid_idx += 1\n",
        "    \n",
        "    train_ds = TensorDataset(x_train[:train_idx], y_train[:train_idx])\n",
        "    valid_ds = TensorDataset(x_valid[:valid_idx], y_valid[:valid_idx])\n",
        "    \n",
        "    return train_ds, valid_ds\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lblh9GDmNdy4",
        "colab_type": "code",
        "outputId": "1e0fa66e-463a-4107-f2dc-4827a6fe4b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "x, y = get_x_y_tensors(df, vocab)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b4dfe5c8bba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x_y_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_x_y_tensors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvkWAx6WfL1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(base_path/'x_tensors.pkl', 'wb') as f:\n",
        "    pickle.dump(x, f)\n",
        "\n",
        "with open(base_path/'y_tensors.pkl', 'wb') as f:\n",
        "    pickle.dump(y, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XtZB2b7fWxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_saved_tensors():\n",
        "    x = None\n",
        "    y = None\n",
        "    with open(base_path/'x_tensors.pkl', 'rb') as f:\n",
        "        x = pickle.load(f)\n",
        "    with open(base_path/'y_tensors.pkl', 'rb') as f:\n",
        "        y = pickle.load(f)\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b66kfmsccrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = get_saved_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YGNlOu9OSnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbMuuNXb7pRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87d0f46b-a9aa-4b73-d4a3-4d959990d0f5"
      },
      "source": [
        "len(train_ds) + len(valid_ds) == len(x)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ_2lRFd8QkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "22da9caa-4142-4d11-eeec-e1a587649347"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e5cb4b876385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miyd_g3mdFub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, encoder_emb, decoder_emb, num_hidden, output_length, num_layers, pad_idx):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.pad_idx = pad_idx\n",
        "        self.output_length = output_length\n",
        "        \n",
        "        self.encoder_emb_size = encoder_emb.embedding_dim\n",
        "        self.decoder_emb_size = decoder_emb.embedding_dim\n",
        "        self.decoder_vocab_size = decoder_emb.num_embeddings\n",
        "        \n",
        "        self.encoder_emb = encoder_emb\n",
        "        self.encoder_emb_drop = nn.Dropout(0.15)\n",
        "        self.encoder_gru = nn.GRU(self.encoder_emb_size, self.num_hidden, num_layers = self.num_layers, dropout = 0.25, batch_first = True)\n",
        "        self.encoder_out = nn.Linear(self.num_hidden, self.decoder_emb_size, bias = False)\n",
        "        \n",
        "        self.decoder_emb = decoder_emb\n",
        "        self.decoder_gru = nn.GRU(self.decoder_emb_size, self.decoder_emb_size, num_layers = self.num_layers, dropout = 0.1, batch_first = True)\n",
        "        self.out_drop = nn.Dropout(0.35)\n",
        "        self.out = nn.Linear(self.decoder_emb_size, self.decoder_vocab_size)\n",
        "        self.out.weight.data = self.decoder_emb.weight.data\n",
        "        \n",
        "    def encoder(self, bs, inp):\n",
        "        h = self.init_hidden(bs)\n",
        "        emb = self.encoder_emb_drop(self.encoder_emb(inp))\n",
        "        _, h = self.encoder_gru(emb, h)\n",
        "        h = self.encoder_out(h)\n",
        "        return h\n",
        "    \n",
        "    def decoder(self, decoder_inp, h):\n",
        "        emb = self.decoder_emb(decoder_inp).unsqueeze(1)\n",
        "        #print(\"decoder emb shape: \" + str(emb.shape))\n",
        "        out_pred, h = self.decoder_gru(emb, h)\n",
        "        #print(\"out_pred shape: \" + str(out_pred.shape))\n",
        "        out_pred = self.out(self.out_drop(out_pred[:,0]))\n",
        "        return h, out_pred\n",
        "        \n",
        "    def forward(self, inp, eos_index):\n",
        "        #print(\"input shape: \" + str(inp.shape))\n",
        "        bs, seq_len = inp.size()\n",
        "        h = self.encoder(bs, inp)\n",
        "        #print(\"hidden shape: \" + str(h.shape))\n",
        "        dec_inp = inp.new_zeros(bs).long()\n",
        "        res = []\n",
        "        for i in range(self.output_length):\n",
        "            #print('i: ' + str(i))\n",
        "            h, out_pred = self.decoder(dec_inp, h)\n",
        "            dec_inp = out_pred.max(1)[1]\n",
        "            res.append(out_pred)\n",
        "            if (dec_inp==eos_index).all(): break\n",
        "        return torch.stack(res, dim = 1)\n",
        "        \n",
        "    def init_hidden(self, bs): return next(self.parameters()).new_zeros(self.num_layers, bs, self.num_hidden)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xna411j0Qnmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_loss(out, targ, pad_idx):\n",
        "    bs,targ_len = targ.size()\n",
        "    #print(\"targ size: \" + str(targ.size()))\n",
        "    _,out_len, vs = out.size()\n",
        "    #print(\"out size \" + str(out.size()))\n",
        "    if targ_len > out_len: out  = F.pad(out,  (0, 0, 0, targ_len - out_len, 0, 0), value=pad_idx)\n",
        "    if out_len > targ_len: targ = F.pad(targ, (0, out_len - targ_len, 0, 0), value=pad_idx)\n",
        "        \n",
        "    #print(\"targ size: \" + str(targ.shape))\n",
        "    #print(\"out size \" + str(out.shape) )\n",
        "    return CrossEntropyFlat()(out.float().cuda(), targ.cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a148UbSuf-Pn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "014883e0-7e64-4ab6-da3c-164d59758593"
      },
      "source": [
        "train_ds, valid_ds = split_train_valid(x, y)\n",
        "train_dl = DataLoader(train_ds, batch_size = 128, shuffle = True, num_workers = 4)\n",
        "valid_dl = DataLoader(valid_ds, batch_size = 128, shuffle = True, num_workers = 4)\n",
        "anti_vocab = generate_anti_vocab(vocab)\n",
        "num_epochs = 100\n",
        "lr = 3e-3\n",
        "model = Seq2SeqModel(enc_emb, dec_emb, 128, 30, 2, vocab['xxxpad']).cuda()\n",
        "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "       \n",
        "        pred = model(xb.cuda(), vocab['xxxeos'])\n",
        "        print(tensor_to_str(pred[0].argmax(1), anti_vocab))\n",
        "        print(tensor_to_str(yb[0], anti_vocab))\n",
        "        loss = seq2seq_loss(pred, yb, vocab['xxxpad'])\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        #TODO: do an accuracy check with validation set\n",
        "    print('Epoch: {0}. Train loss: {1}.'.format(epoch + 1, total_loss/len(train_dl)))\n",
        "        "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['they', 'they', 'disgraceful', 'turnoff', 'voltaire', 'silence', 'recklessness', \"where'm\", 'yeah', 'taco', 'fairytale', 'señor', 'disgraceful', 'ejaculate', 'i', 'the', 'excuse', 'disgraceful', 'taco', 'señor', 'disgraceful', 'he', 'yeah', 'taco', 'disgraceful', 'disgraceful', 'señor', 'disgraceful', ',', 'señor']\n",
            "['your', 'choice', '.', 'look', 'at', 'the', 'time', '!', 'come', '.', 'there', \"'s\", 'someone', 'i', 'want', 'you', 'meet', '-', 'about', 'a', 'story', 'i', \"'m\", 'thinking', 'of', 'publishing', '.', 'xxxeos', 'xxxpad', 'xxxpad']\n",
            "['bench', 'hejira', 'fron', 'uh', 'morg', 'cuter', 'you', 'i', 'morg', 'i', ',', 'i', ',', ',', ',', 'wright', 'rallying', ',', 'wright', 'wright', 'i', 'wright', 'the', 'the', ',', 'taco', 'rinaldi', ',', 'i', ',']\n",
            "['what', 'is', 'that', '?', 'what', 'are', 'you', 'doing', 'with', 'your', 'hands', '?', 'talk', 'to', 'me', ',', 'you', \"'re\", 'talking', 'like', 'that', 'girl', 'sheila', '.', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['xxxeos', 'i', 'i\\x92ll', 'siberia', 'fiancé', '?', '?', 'i', '?', 'arranged', 'xxxpad', '?', '24-hour', 'liquor', '...', 'directions', 'holland', '...', 'lend', 'guide', 'buckley', 'built', 'sure', 'who', 'cc', 'a', 'chinaman', 'shop', 'yanks', 'patterns']\n",
            "['fucked', 'if', 'i', 'care', ',', 'man', '.', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['xxxpad', 'was', 'xxxpad', 'feline', 'possession', '.', \"'s\", \"'s\", 'what', 'send', \"'s\", '.', \"'s\", 'taco', 'intrigue', 'chained', 'danger</u', 'eternal', 'misty', 'tully', 'use', 'xxxpad', \"'s\", 'xxxpad', \"'s\", 'taco', '85', \"'s\", 'foo', '85']\n",
            "['oh', ',', 'yeah', ',', 'i', \"'m\", 'calm', '.', 'in', 'fact', ',', 'i', \"'m\", 'totally', '\"', 'relaxed', '...', '\"', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', \"'s\", 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'problem', 'xxxpad', 'xxxpad', 'what', 'xxxpad', 'xxxpad', 'calm</u', 'xxxpad', 'xxxpad', 'xxxpad', '.', 'xxxpad', 'xxxpad']\n",
            "['yes', '.', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['fucked', 'our', 'not', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'with', 'xxxpad', 'the', 'they', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'lizy', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['why', 'do', 'i', 'let', 'you', 'talk', 'me', 'into', 'these', 'things', '?', 'why', '?', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['normal', '--', 'as', '...', 'xxxpad', 'xxxpad', 'xxxpad', 'shit', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', '.', 'in', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'not', 'xxxpad', 'xxxpad', 'all', 'xxxpad', 'xxxpad', 'xxxpad', 'days']\n",
            "['xxxunk', 'zogg', ',', 'how', 'would', 'you', 'like', 'your', 'black', 'butt', 'slung', 'into', 'a', 'general', 'court', 'martial', 'when', 'we', 'get', 'back', '?', '?', '?', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['allman', 'i', 'other', '-', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'and', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'i']\n",
            "['yeah', ',', 'sure', '.', 'go', 'ahead', 'and', 'plan', 'it', '...', 'for', 'a', 'year', 'or', 'two', '.', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['the', 'and', 'be', 'i', 'moorish', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'those', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxeos', 'xxxpad', 'should', 'xxxpad', 'xxxpad', 'xxxpad', 'with', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['look', 'after', 'that', 'suit', ',', 'eh', '.', 'barbara', 'chose', 'it', 'for', 'me', '.', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['sonuvabuck', 'xxxeos', 'believe', 'much', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxeos', 'xxxpad', 'xxxpad', 'and', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['i', \"'m\", 'by', 'way', 'of', 'being', 'a', 'medical', 'myself', '.', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "[',', 'xxxeos', 'better', 'special', 'room', 'and', 'was', 'xxxpad', '.', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'me', 'xxxpad', 'stand', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['i', 'had', 'a', 'witness', 'i', 'would', '.', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['unattractive', 'turner', 'trophy', 'absolutely', 'i', 'work-', '.', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxeos', 'xxxpad', 'xxxpad', 'you', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', '.', 'xxxpad', 'xxxpad']\n",
            "['how', 'much', 'is', 'your', 'soul', 'worth', 'if', 'you', 'do', \"n't\", '?', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['students', 'reconstruction', 'yeah', '.', '...', '.', 'xxxpad', 'xxxpad', 'hair', '!', '!', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'they', 'have', 'xxxpad', 'mine', 'xxxpad', '_', 'xxxpad', 'xxxpad', 'attack', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['of', 'course', 'you', 'are', '.', 'and', 'what', 'a', 'grand', 'pursuit', 'you', 'must', 'be', '.', 'what', 'do', 'you', 'think', 'of', 'my', 'new', 'invention', '?', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['xxxeos', 'door', \"'s\", 'you', 'xxxpad', 'some', 'xxxpad', 'xxxpad', 'now', 'xxxpad', 'xxxpad', 'that', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'not', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['sir', '?', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['melodrama', 'hey', 'implanted', 'now', 'now', 'now', 'now', 'now', 'now', 'at', 'now', 'phone', 'now', 'me', 'boyd', 'much', 'calm</u', 'sap', 'xxxpad', 'xxxeos', 'abhor', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['he', \"'s\", 'in', 'there', '...', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['do', 'luca', 'xxxpad', 'few', '?', 'xxxpad', 'out', 'or', 'all', 'xxxpad', 'that', 'xxxpad', 'xxxpad', 'day', 'xxxpad', 'that', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['emily', '-', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['the', 'shoe', 'hi', 'xxxpad', 'subject', 'morg', 'you', 'xxxeos', 'xxxpad', 'great', 'mine', 'xxxpad', 'you', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'happen', 'xxxpad', 'xxxpad', 'gilbert', 'xxxpad', 'those', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n",
            "['yes', ',', 'sir', '!', 'haaa', '!', 'xxxeos', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad', 'xxxpad']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-87cf89ddf2cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xxxpad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6wLMt7MR3mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), base_path/'fake.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSIQQ7Dx8YoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "76d0ec24-4301-4d41-aea5-992720440cd9"
      },
      "source": [
        "model = Seq2SeqModel(enc_emb, dec_emb, 128, 30, 2, vocab['xxxpad']).cuda()\n",
        "model.load_state_dict(torch.load(base_path/'seq2seq.pt'))\n",
        "model.eval()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqModel(\n",
              "  (encoder_emb): Embedding(32653, 100, padding_idx=0)\n",
              "  (encoder_emb_drop): Dropout(p=0.15)\n",
              "  (encoder_gru): GRU(100, 128, num_layers=2, batch_first=True, dropout=0.25)\n",
              "  (encoder_out): Linear(in_features=128, out_features=100, bias=False)\n",
              "  (decoder_emb): Embedding(32653, 100, padding_idx=0)\n",
              "  (decoder_gru): GRU(100, 100, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (out_drop): Dropout(p=0.35)\n",
              "  (out): Linear(in_features=100, out_features=32653, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmL6vq-tODro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_anti_vocab(vocab):\n",
        "    res = dict()\n",
        "    for key in vocab:\n",
        "        res[vocab[key]] = key\n",
        "    return res\n",
        "\n",
        "def str_to_tensor(inp, vocab):\n",
        "    tokens = tokenize(preprocess(inp), vocab = vocab)\n",
        "    res = torch.zeros(len(tokens))\n",
        "    for i in range(len(tokens)):\n",
        "        res[i] = vocab[tokens[i]]\n",
        "    return res.unsqueeze(0).long()\n",
        "    \n",
        "def tensor_to_str(inp, anti_vocab):\n",
        "    res = []\n",
        "    for i in range(len(inp)):\n",
        "        res.append(anti_vocab[inp[i].item()])\n",
        "    return res\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbdJrA2Axq6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anti_vocab = generate_anti_vocab(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6f0Ov9exuE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = str_to_tensor(\"hi there, I am a friend of yours\", vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BwgKHZ_x1nY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb59555a-9441-4eb2-faed-992519f68edb"
      },
      "source": [
        "pred = model(t.cuda(), vocab['xxxeos'])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n",
            "tensor([[ 6.7114,  5.3207, -9.9145,  ..., -9.8984, -4.3586, -6.7311]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zatX2pFQx5ps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28310a66-2902-4ee5-d006-4bc2fad0c937"
      },
      "source": [
        "pred.shape, t.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 30, 32653]), torch.Size([1, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t6GTIRzzbHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2bfcf6c9-0cde-4fab-c1ed-7e07acaff79d"
      },
      "source": [
        "pred.argmax(2)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA7FWAin9rO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}