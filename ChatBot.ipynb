{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewPochapsky/chatbot/blob/master/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0I5A7ULV5Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from ast import literal_eval\n",
        "from fastai.layers import CrossEntropyFlat\n",
        "import spacy\n",
        "import pickle\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS74TvsTbjQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = Path('drive/My Drive/datasets/cornell movie-dialogs corpus')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRhpvb63aYdk",
        "colab_type": "code",
        "outputId": "6bfdb02e-e6f9-4817-a806-161737a7327b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc78yc7jY3To",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTVhRWBJkSbX",
        "colab_type": "text"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthfvrbqY7hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(s):\n",
        "    s = s.replace('\\n',' ').lower()\n",
        "    return s\n",
        "\n",
        "def tokenize(corpus, vocab = None):\n",
        "    tokenizer = spacy.blank(\"en\").tokenizer\n",
        "    doc = tokenizer(corpus)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if(token.text.strip() != \"\"):\n",
        "            if(vocab != None and token.text not in vocab):\n",
        "                tokens.append('xxxunk')\n",
        "            else:\n",
        "                tokens.append(token.text)\n",
        "    return tokens\n",
        "\n",
        "def process_dataset():\n",
        "    all_words = \"\"\n",
        "    with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            all_words += parts[-1]\n",
        "    return all_words\n",
        "\n",
        "def generate_vocab(tokens, min_freq = 0):\n",
        "    all_unique_words_counter = Counter(tokens)\n",
        "    vocab = {}\n",
        "    vocab['xxxpad'] = 0\n",
        "    vocab['xxxeos'] = 1\n",
        "    vocab['xxxbos'] = 2\n",
        "    vocab['xxxunk'] = 3\n",
        "    index = 4\n",
        "    for w in all_unique_words_counter.keys():\n",
        "        if(all_unique_words_counter[w] >= min_freq and w.strip() != \"\"):\n",
        "            vocab[w] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "def replace_with_unk(tokens, vocab):\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i] if tokens[i] in vocab else 'xxxunk'\n",
        "        \n",
        "\n",
        "def subsample(tokens, t = 1e-5):\n",
        "    \"\"\"\n",
        "        Paper: https://arxiv.org/pdf/1310.4546.pdf\n",
        "    \"\"\"\n",
        "    sampled_tokens = []\n",
        "    counter = Counter(tokens)\n",
        "    for token in tokens:\n",
        "        f_w = counter[token]/len(tokens)\n",
        "        p_w = 1 - math.sqrt(t/f_w)\n",
        "        val = random.uniform(0, 1)\n",
        "        if(val >= p_w):\n",
        "            sampled_tokens.append(token)\n",
        "            \n",
        "    return sampled_tokens\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "def create_training_matrices(vocab, all_words, window_size = 5):\t\n",
        "\t\"\"\"\n",
        "        Returns x_train: Tensor()\n",
        "    \"\"\"\n",
        "\tnumTotalWords = len(all_words)\n",
        "\txTrain=[]\n",
        "\tyTrain=[]\n",
        "\tfor i in range(numTotalWords):\n",
        "\t\twordsAfter = all_words[i + 1:i + window_size + 1]\n",
        "\t\twordsBefore = all_words[max(0, i - window_size):i]\n",
        "\t\twordsAdded = wordsAfter + wordsBefore\n",
        "\t\tfor word in wordsAdded:\n",
        "\t\t\txTrain.append(vocab[all_words[i]])\n",
        "\t\t\tyTrain.append(vocab[word])\n",
        "\treturn Tensor(xTrain), Tensor(yTrain)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUKWTcYDu9_j",
        "colab_type": "code",
        "outputId": "7419b10e-035b-4f87-9c7d-99b8ea58423e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "\n",
        "full_corpus = process_dataset() \n",
        "full_corpus = preprocess(full_corpus)\n",
        "print('Begin Tokenization')\n",
        "tokens = tokenize(full_corpus) # list of string\n",
        "print('Generating vocab')\n",
        "vocab = generate_vocab(tokens, min_freq = 2)\n",
        "\n",
        "replace_with_unk(tokens, vocab)\n",
        "\n",
        "print(len(tokens))\n",
        "print('Subsampling data')\n",
        "tokens = subsample(tokens, 1e-5)\n",
        "print(len(tokens))\n",
        "print('Getting training data')\n",
        "x_train, y_train = create_training_matrices(vocab, tokens, window_size = 3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Tokenization\n",
            "Generating vocab\n",
            "4194111\n",
            "Subsampling data\n",
            "665862\n",
            "Getting training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94YV0Yi-2rnJ",
        "colab_type": "text"
      },
      "source": [
        "SkipGram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avoAJTVmuGPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, emb_size, emb_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.center_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.context_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.init_emb()\n",
        "    \n",
        "    def init_emb(self):\n",
        "        #initrange = 0.5 / self.emb_dim\n",
        "        #self.center_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        nn.init.kaiming_uniform_(self.center_embeddings.weight, a=math.sqrt(5))\n",
        "        self.context_embeddings.weight.data.uniform_(-0, 0)\n",
        "        \n",
        "    def forward(self, pos_center, pos_context, neg_context):\n",
        "        losses = []\n",
        "        emb_center = self.center_embeddings(pos_center.long())\n",
        "        emb_context = self.context_embeddings(pos_context.long())\n",
        "        score = torch.mul(emb_center, emb_context).squeeze()\n",
        "        #print(score.shape)\n",
        "        score = torch.sum(score, dim = 1)\n",
        "        score = F.logsigmoid(score) # I think it is logsigmoid since we are doing nll loss func?\n",
        "        losses.append(sum(score))\n",
        "        \n",
        "        \n",
        "        neg_emb_context = self.context_embeddings(neg_context.long())\n",
        "        #print(neg_emb_context.shape)\n",
        "        neg_score = torch.bmm(neg_emb_context, emb_center.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.sum(neg_score, dim = 1)\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        losses.append(sum(neg_score))\n",
        "        return -1 * sum(losses)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QZ94UCm_pfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size = 128, shuffle = True, num_workers = 4)\n",
        "\n",
        "def map_to_index(np_array, vocab):\n",
        "    output = torch.zeros(np_array.shape)\n",
        "    for i in range(len(np_array)):\n",
        "        output[i] = Tensor(list(map(lambda x: vocab[x], np_array[i])))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_JMxP2N2-al",
        "colab_type": "code",
        "outputId": "478d3a7d-ca3c-4eb3-fc05-6b4365efdc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "lr = 3e-3\n",
        "\n",
        "num_epochs = 10\n",
        "neg_sample_size = 5\n",
        "emb_dim = 100\n",
        "emb_size = len(vocab.keys())\n",
        "model = SkipGramModel(emb_size, emb_dim)\n",
        "model.load_state_dict(torch.load(base_path/'word2vec0.pt'))\n",
        "optim = torch.optim.SGD(model.parameters(), lr = lr) #cant use mom or wd since that would require calculating for all the params, too expensive\n",
        "i = 0\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    i = 0\n",
        "    for xb, yb in train_dl:\n",
        "        #neg sampling\n",
        "        neg_context = np.random.choice(\n",
        "            tokens,\n",
        "            size=(len(xb), neg_sample_size)\n",
        "        )\n",
        "        \n",
        "        neg_context = map_to_index(neg_context, vocab)\n",
        "        #print(type(neg_context))\n",
        "        optim.zero_grad()\n",
        "        loss = model(xb, yb, neg_context)\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if(i % 100 == 0):\n",
        "            print('completed {0}/{1} batches. Avg loss per batch: {2}'.format(i, len(train_dl), total_loss/(i)))\n",
        "        i+=1\n",
        "    \n",
        "    torch.save(model.state_dict(), base_path/'word2vec{0}.pt'.format(epoch))\n",
        "    \n",
        "    \n",
        "torch.save(model.state_dict(), base_path/'word2vec.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed 0/31234 batches. Avg loss per batch: inf\n",
            "completed 100/31234 batches. Avg loss per batch: 153.0912322998047\n",
            "completed 200/31234 batches. Avg loss per batch: 152.4522247314453\n",
            "completed 300/31234 batches. Avg loss per batch: 152.2268524169922\n",
            "completed 400/31234 batches. Avg loss per batch: 152.18731689453125\n",
            "completed 500/31234 batches. Avg loss per batch: 152.10638427734375\n",
            "completed 600/31234 batches. Avg loss per batch: 152.03564453125\n",
            "completed 700/31234 batches. Avg loss per batch: 151.99029541015625\n",
            "completed 800/31234 batches. Avg loss per batch: 151.93702697753906\n",
            "completed 900/31234 batches. Avg loss per batch: 151.91429138183594\n",
            "completed 1000/31234 batches. Avg loss per batch: 151.90428161621094\n",
            "completed 1100/31234 batches. Avg loss per batch: 151.91932678222656\n",
            "completed 1200/31234 batches. Avg loss per batch: 151.88522338867188\n",
            "completed 1300/31234 batches. Avg loss per batch: 151.81710815429688\n",
            "completed 1400/31234 batches. Avg loss per batch: 151.77047729492188\n",
            "completed 1500/31234 batches. Avg loss per batch: 151.72305297851562\n",
            "completed 1600/31234 batches. Avg loss per batch: 151.7111053466797\n",
            "completed 1700/31234 batches. Avg loss per batch: 151.68276977539062\n",
            "completed 1800/31234 batches. Avg loss per batch: 151.62599182128906\n",
            "completed 1900/31234 batches. Avg loss per batch: 151.58807373046875\n",
            "completed 2000/31234 batches. Avg loss per batch: 151.54115295410156\n",
            "completed 2100/31234 batches. Avg loss per batch: 151.49571228027344\n",
            "completed 2200/31234 batches. Avg loss per batch: 151.45960998535156\n",
            "completed 2300/31234 batches. Avg loss per batch: 151.42123413085938\n",
            "completed 2400/31234 batches. Avg loss per batch: 151.3830108642578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-137a60546c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFZBp1wWtt8U",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcMUbFbcGTea",
        "colab_type": "text"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukHUcLxPGQdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line_map = {}\n",
        "with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        line_num = parts[0]\n",
        "        #-2 to get rid of \\n\n",
        "        text = parts[-1][:-1]\n",
        "        line_map[line_num] = text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0opgwHBGRGH",
        "colab_type": "code",
        "outputId": "d81e72b3-5dcc-42fa-c8cb-7df64f08abab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "table = []\n",
        "with open(base_path/'movie_conversations.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        #get the referenced line numbers\n",
        "        line_nums = re.findall('L[0-9]+', parts[-1])\n",
        "        #form pairs\n",
        "        \n",
        "        for i in range(len(line_nums) - 1):\n",
        "            pair = (line_nums[i], line_nums[i+1])\n",
        "            #df.loc[df['column_name'] == some_value]\n",
        "            first = line_map[line_nums[i]]\n",
        "            second = line_map[line_nums[i+1]]\n",
        "            table.append([tokenize(preprocess(first), vocab = vocab), tokenize(preprocess(second), vocab = vocab)])\n",
        "        \n",
        "data_df = pd.DataFrame(table, columns = ['in', 'out'])\n",
        "data_df.to_csv(base_path/'processed_data.csv', index = False)\n",
        "data_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>in</th>\n",
              "      <th>out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[can, we, make, this, quick, ?, xxxunk, xxxunk...</td>\n",
              "      <td>[well, ,, i, thought, we, 'd, start, with, xxx...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[well, ,, i, thought, we, 'd, start, with, xxx...</td>\n",
              "      <td>[not, the, hacking, and, gagging, and, spittin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[not, the, hacking, and, gagging, and, spittin...</td>\n",
              "      <td>[okay, ..., then, how, 'bout, we, try, out, so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[you, 're, asking, me, out, ., that, 's, so, c...</td>\n",
              "      <td>[forget, it, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[no, ,, no, ,, it, 's, my, fault, --, we, did,...</td>\n",
              "      <td>[cameron, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  in                                                out\n",
              "0  [can, we, make, this, quick, ?, xxxunk, xxxunk...  [well, ,, i, thought, we, 'd, start, with, xxx...\n",
              "1  [well, ,, i, thought, we, 'd, start, with, xxx...  [not, the, hacking, and, gagging, and, spittin...\n",
              "2  [not, the, hacking, and, gagging, and, spittin...  [okay, ..., then, how, 'bout, we, try, out, so...\n",
              "3  [you, 're, asking, me, out, ., that, 's, so, c...                                    [forget, it, .]\n",
              "4  [no, ,, no, ,, it, 's, my, fault, --, we, did,...                                       [cameron, .]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFiis_fjY2b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def converter(x):\n",
        "    #convert \"list\" to list\n",
        "    return literal_eval(x)\n",
        "\n",
        "converters={'in': converter, 'out': converter}\n",
        "df = pd.read_csv(base_path/'processed_data.csv', converters = converters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Dj7BZNGYdw",
        "colab_type": "text"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ocnidotyGS",
        "colab_type": "code",
        "outputId": "5637bf96-2ea8-4dc2-a082-ab7b0dea5f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "emb_dim = 100\n",
        "emb_size = len(vocab.keys())\n",
        "word2vec_model = SkipGramModel(emb_size, emb_dim)\n",
        "word2vec_model.load_state_dict(torch.load(base_path/'word2vec0.pt'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ifkijpHmkN",
        "colab_type": "code",
        "outputId": "ec7da498-82be-45e2-df5c-46242971c360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.percentile([len(row['in']) for index, row in df.iterrows()], 90)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RMlUMGdEnAl",
        "colab_type": "code",
        "outputId": "3b3d47d2-282a-4873-93bf-90ebe3199729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.percentile([len(row['out']) for index, row in df.iterrows()], 90)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hE7FEaYIMBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(word2vec_model, emb_size, emb_dim, padding_idx, eos_idx):\n",
        "    new_emb = nn.Embedding(emb_size, emb_dim, padding_idx=padding_idx)\n",
        "    old_emb_weights = word2vec_model.center_embeddings.weight.data\n",
        "    for i in range(len(old_emb_weights)):\n",
        "        new_emb.weight.data[i] = old_emb_weights[i]\n",
        "    \n",
        "    #init the new embeddings to zero\n",
        "    new_emb.weight.data[padding_idx].uniform_(-old_emb_weights.mean(), old_emb_weights.mean())\n",
        "    new_emb.weight.data[eos_idx].uniform_(-old_emb_weights.mean(), old_emb_weights.mean())\n",
        "    \n",
        "    return new_emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v2JG88euzjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_emb = create_embedding_matrix(word2vec_model, emb_size, emb_dim, vocab['xxxpad'], vocab['xxxeos'])    \n",
        "dec_emb = create_embedding_matrix(word2vec_model, emb_size, emb_dim, vocab['xxxpad'], vocab['xxxeos'])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106s1eIDb2rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_x_and_y(df):\n",
        "    max_x = -1\n",
        "    for x in df['in']:\n",
        "        if(len(x) > max_x):\n",
        "            max_x = len(x)\n",
        "            \n",
        "    max_y = -1\n",
        "    for y in df['out']:\n",
        "        if(len(y) > max_y):\n",
        "            max_y = len(y)\n",
        "    max_y += 1\n",
        "    with open(base_path/'max.txt', 'w+') as f:\n",
        "        f.write(str(max_x) + ',' + str(max_y))\n",
        "    \n",
        "    return max_x, max_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSme7GyGcKnj",
        "colab_type": "code",
        "outputId": "d34a909d-1935-42e5-e04a-fae7d60a584b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_max_x_and_y(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(370, 683)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO1om97EcNQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_x = None\n",
        "max_y = None\n",
        "with open(base_path/'max.txt') as f:\n",
        "    for line in f:\n",
        "        max_x, max_y = line.split(',')\n",
        "        max_x = int(max_x)\n",
        "        max_y = int(max_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfoUy4ZNwXV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_x_y_tensors(df, vocab, max_x = 30, max_y = 30):\n",
        "    num_rows = len(df.index)\n",
        "    num_valid_samples = 0\n",
        "    for index, row in df.iterrows():\n",
        "        #do < instead of <= so there is room for a token if I choose to add\n",
        "        if(len(row['in']) < max_x and len(row['out']) < max_y):\n",
        "            num_valid_samples += 1\n",
        "    res_x = torch.zeros(num_valid_samples, max_x + 2).long() \n",
        "    res_y = torch.zeros(num_valid_samples, max_y + 2).long()\n",
        "    tensor_idx = 0\n",
        "    for row_idx in range(num_rows):\n",
        "        if(row_idx % 10000 == 0): print('done {0}/{1} samples'.format(row_idx, num_rows))\n",
        "        \n",
        "        x, y = df.iloc[row_idx, :]\n",
        "        \n",
        "        if(len(x) >= max_x  or len(y) >= max_y ): continue\n",
        "        \n",
        "        x_tensor = torch.zeros(max_x + 2) + vocab['xxxpad']\n",
        "        y_tensor = torch.zeros(max_y + 2) + vocab['xxxpad']\n",
        "\n",
        "        num_padding = max_x - len(x)\n",
        "         \n",
        "        #populate the rest of it with actual input\n",
        "        token_index = 0;\n",
        "        for i in range(num_padding, max_x):\n",
        "            x_tensor[i] = vocab[x[token_index]] \n",
        "            token_index += 1\n",
        "               \n",
        "        #Add tokens\n",
        "        x_tensor[num_padding - 1] = vocab['xxxbos']\n",
        "        x_tensor[-1] = vocab['xxxeos']\n",
        "            \n",
        "        #add input to the output\n",
        "        for i in range(1, len(y)):\n",
        "            y_tensor[i] = vocab[y[i - 1]]\n",
        "            \n",
        "        #add tokens\n",
        "        y_tensor[0] = vocab['xxxbos']\n",
        "        y_tensor[len(y)] = vocab['xxxeos']\n",
        "            \n",
        "        res_x[tensor_idx] = x_tensor.long()\n",
        "        res_y[tensor_idx] = y_tensor.long()\n",
        "        tensor_idx += 1\n",
        "        \n",
        "    return res_x, res_y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUHOYTLaun0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x and y obtained from get_x_y_tensors()\n",
        "def split_train_valid(all_x, all_y, valid_pct = 0.2):\n",
        "    num_x_samples, x_len = all_x.shape\n",
        "    num_y_samples, y_len = all_y.shape\n",
        "    \n",
        "    assert num_x_samples == num_y_samples\n",
        "    \n",
        "    x_train = torch.zeros(num_x_samples, x_len).long()\n",
        "    y_train = torch.zeros(num_y_samples, y_len).long()\n",
        "    \n",
        "    x_valid = torch.zeros(num_x_samples, x_len).long()\n",
        "    y_valid = torch.zeros(num_y_samples, y_len).long()\n",
        "    \n",
        "    train_idx = 0\n",
        "    valid_idx = 0\n",
        "    \n",
        "    for x, y in zip(all_x, all_y):\n",
        "        rand_num = random.uniform(0, 1)\n",
        "        if(rand_num >= 0.2):\n",
        "            x_train[train_idx] = x.squeeze(0).long()\n",
        "            y_train[train_idx] = y.squeeze(0).long()\n",
        "            train_idx += 1\n",
        "        else:\n",
        "            x_valid[valid_idx] = x.squeeze(0).long()\n",
        "            y_valid[valid_idx] = y.squeeze(0).long()\n",
        "            valid_idx += 1\n",
        "\n",
        "    train_ds = TensorDataset(x_train[:train_idx], y_train[:train_idx])\n",
        "    valid_ds = TensorDataset(x_valid[:valid_idx], y_valid[:valid_idx])\n",
        "    \n",
        "    return train_ds, valid_ds\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lblh9GDmNdy4",
        "colab_type": "code",
        "outputId": "0a84f021-dbf3-470f-e6b5-058fe2b3b6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "x, y = get_x_y_tensors(df, vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 0/221616 samples\n",
            "done 10000/221616 samples\n",
            "done 20000/221616 samples\n",
            "done 30000/221616 samples\n",
            "done 40000/221616 samples\n",
            "done 50000/221616 samples\n",
            "done 60000/221616 samples\n",
            "done 70000/221616 samples\n",
            "done 80000/221616 samples\n",
            "done 90000/221616 samples\n",
            "done 100000/221616 samples\n",
            "done 110000/221616 samples\n",
            "done 120000/221616 samples\n",
            "done 130000/221616 samples\n",
            "done 140000/221616 samples\n",
            "done 150000/221616 samples\n",
            "done 160000/221616 samples\n",
            "done 170000/221616 samples\n",
            "done 180000/221616 samples\n",
            "done 190000/221616 samples\n",
            "done 200000/221616 samples\n",
            "done 210000/221616 samples\n",
            "done 220000/221616 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvkWAx6WfL1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(base_path/'x_tensors.pkl', 'wb') as f:\n",
        "    pickle.dump(x, f)\n",
        "\n",
        "with open(base_path/'y_tensors.pkl', 'wb') as f:\n",
        "    pickle.dump(y, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XtZB2b7fWxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_saved_tensors():\n",
        "    x = None\n",
        "    y = None\n",
        "    with open(base_path/'x_tensors.pkl', 'rb') as f:\n",
        "        x = pickle.load(f)\n",
        "    with open(base_path/'y_tensors.pkl', 'rb') as f:\n",
        "        y = pickle.load(f)\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b66kfmsccrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = get_saved_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj_Zhx5CZMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6352125d-9e3b-4d32-d310-ff027df068ae"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([181652, 32]), torch.Size([181652, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMdRAwsADQEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d8889ec5-ac12-4780-eb08-6f75713c1ab1"
      },
      "source": [
        "x"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,    0,    0,  ...,   12,    0,    1],\n",
              "        [   0,    0,    0,  ...,   12,    0,    1],\n",
              "        [   0,    0,    0,  ...,   12,    0,    1],\n",
              "        ...,\n",
              "        [   0,    0,    0,  ...,   12,    0,    1],\n",
              "        [   0,    0,    0,  ..., 2117,    0,    1],\n",
              "        [   0,    0,    0,  ...,   15,    0,    1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miyd_g3mdFub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, encoder_emb, decoder_emb, num_hidden, output_length, num_layers, pad_idx):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.pad_idx = pad_idx\n",
        "        self.output_length = output_length\n",
        "        \n",
        "        self.encoder_emb_size = encoder_emb.embedding_dim\n",
        "        self.decoder_emb_size = decoder_emb.embedding_dim\n",
        "        self.decoder_vocab_size = decoder_emb.num_embeddings\n",
        "        \n",
        "        self.encoder_emb = encoder_emb\n",
        "        self.encoder_emb_drop = nn.Dropout(0.15)\n",
        "        self.encoder_gru = nn.GRU(self.encoder_emb_size, self.num_hidden, num_layers = self.num_layers, dropout = 0.25, batch_first = True, bidirectional = True)\n",
        "        self.encoder_out = nn.Linear(self.num_hidden * 2, self.decoder_emb_size, bias = False)\n",
        "        \n",
        "        self.decoder_emb = decoder_emb\n",
        "        self.decoder_gru = nn.GRU(self.decoder_emb_size, self.decoder_emb_size, num_layers = self.num_layers, dropout = 0.1, batch_first = True)\n",
        "        self.out_drop = nn.Dropout(0.35)\n",
        "        self.out = nn.Linear(self.decoder_emb_size, self.decoder_vocab_size)\n",
        "        self.out.weight.data = self.decoder_emb.weight.data\n",
        "        \n",
        "    def encoder(self, bs, inp):\n",
        "        h = self.init_hidden(bs)\n",
        "        emb = self.encoder_emb_drop(self.encoder_emb(inp))\n",
        "        enc_out, hid = self.encoder_gru(emb, h)\n",
        "        pre_hid = hid.view(2, self.num_layers, bs, self.num_hidden).permute(1,2,0,3).contiguous() # shape becomes (self.num_layers, bs, 2, self.num_hidden)\n",
        "        pre_hid = pre_hid.view(self.num_layers, bs, self.num_hidden * 2)\n",
        "        hid = self.encoder_out(pre_hid)\n",
        "        return hid\n",
        "    \n",
        "    def decoder(self, decoder_inp, h):\n",
        "        emb = self.decoder_emb(decoder_inp).unsqueeze(1)\n",
        "        #print(\"decoder emb shape: \" + str(emb.shape))\n",
        "        out_pred, h = self.decoder_gru(emb, h)\n",
        "        #print(\"out_pred shape: \" + str(out_pred.shape))\n",
        "        out_pred = self.out(self.out_drop(out_pred[:,0]))\n",
        "        return h, out_pred\n",
        "        \n",
        "    def forward(self, inp, eos_index, targ, teach_force):\n",
        "        bs, seq_len = inp.size()\n",
        "        #h is the context\n",
        "        h = self.encoder(bs, inp)\n",
        "        dec_inp = inp.new_zeros(bs).long()\n",
        "        res = []\n",
        "        for i in range(self.output_length):\n",
        "            \n",
        "            h, out_pred = self.decoder(dec_inp, h)\n",
        "            dec_inp = out_pred.max(1)[1]\n",
        "            res.append(out_pred)\n",
        "            if (dec_inp==self.pad_idx).all(): break #TODO: not sure if this should be looking for pad index instead\n",
        "            #Teacher forcing\n",
        "            if(targ is not None and random.random() < teach_force):\n",
        "                dec_inp = targ[:,i]    \n",
        "        return torch.stack(res, dim = 1)\n",
        "        \n",
        "    def init_hidden(self, bs): return next(self.parameters()).new_zeros(2 * self.num_layers, bs, self.num_hidden)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xna411j0Qnmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_loss(out, targ, pad_idx):\n",
        "    bs,targ_len = targ.shape\n",
        "    _,out_len, vs = out.shape\n",
        "    if targ_len > out_len: out  = F.pad(out,  (0, 0, 0, targ_len - out_len, 0, 0), value=pad_idx)\n",
        "    if out_len > targ_len: targ = F.pad(targ, (0, out_len - targ_len, 0, 0), value=pad_idx)\n",
        "    \n",
        "    return CrossEntropyFlat()(out.float().cuda(), targ.cuda())\n",
        "\n",
        "\n",
        "def seq2seq_acc(out, targ, pad_idx):\n",
        "    bs,targ_len = targ.shape\n",
        "    _,out_len, vs = out.shape\n",
        "    if targ_len > out_len: out  = F.pad(out,  (0, 0, 0, targ_len - out_len, 0, 0), value=pad_idx)\n",
        "    if out_len > targ_len: targ = F.pad(targ, (0, out_len - targ_len, 0, 0), value=pad_idx)\n",
        "    \n",
        "    out = out.argmax(2)\n",
        "    return (out==targ.cuda()).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a148UbSuf-Pn",
        "colab_type": "code",
        "outputId": "824e3393-3bdb-4704-e2ef-42c785420b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "train_ds, valid_ds = split_train_valid(x, y)\n",
        "train_dl = DataLoader(train_ds, batch_size = 64, shuffle = True, num_workers = 4)\n",
        "valid_dl = DataLoader(valid_ds, batch_size = 64, shuffle = True, num_workers = 4)\n",
        "#anti_vocab = generate_anti_vocab(vocab)\n",
        "num_epochs = 100\n",
        "lr = 3e-3\n",
        "model = Seq2SeqModel(enc_emb, dec_emb, 256, 30, 2, vocab['xxxpad']).cuda()\n",
        "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "        \n",
        "        pred = model(xb.cuda(), vocab['xxxeos'], yb.cuda(), 1 - epoch/num_epochs)\n",
        "        #print(tensor_to_str(pred[0].argmax(1), anti_vocab))\n",
        "        #print(tensor_to_str(yb[0], anti_vocab))\n",
        "        loss = seq2seq_loss(pred, yb, vocab['xxxpad'])\n",
        "        train_loss += loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        print('ayy')\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        acc = 0\n",
        "        valid_loss = 0\n",
        "        model.eval()\n",
        "        for xb, yb in valid_dl:\n",
        "            pred = model(xb.cuda(), vocab['xxxeos'], None, -1)\n",
        "            valid_loss += seq2seq_loss(pred, yb, vocab['xxxpad'])\n",
        "            acc += seq2seq_acc(pred, yb, vocab['xxxpad'])\n",
        "            \n",
        "        \n",
        "        \n",
        "    print('Epoch: {0}. Train loss: {1}. Valid loss: {2}. Valid accuracy: {3}'.format(epoch + 1, train_loss/len(train_dl), valid_loss/len(valid_dl), acc/len(valid_dl)))\n",
        "        "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8583f33f677e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xxxeos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#print(tensor_to_str(pred[0].argmax(1), anti_vocab))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(tensor_to_str(yb[0], anti_vocab))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a3eddd319705>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, eos_index, targ, teach_force)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mdec_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a3eddd319705>\u001b[0m in \u001b[0;36mdecoder\u001b[0;34m(self, decoder_inp, h)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#print(\"decoder emb shape: \" + str(emb.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mout_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print(\"out_pred shape: \" + str(out_pred.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mout_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise RuntimeError(\n\u001b[1;32m    152\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 153\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 612, got 100"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6wLMt7MR3mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), base_path/'fake.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSIQQ7Dx8YoU",
        "colab_type": "code",
        "outputId": "1295e047-258c-4aee-bbb3-8dbfcabde5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "model = Seq2SeqModel(enc_emb, dec_emb, 256, 30, 2, vocab['xxxeos']).cuda()\n",
        "model.load_state_dict(torch.load(base_path/'seq2seq_19.pt'))\n",
        "model.eval()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqModel(\n",
              "  (encoder_emb): Embedding(32653, 100, padding_idx=0)\n",
              "  (encoder_emb_drop): Dropout(p=0.15)\n",
              "  (encoder_gru): GRU(100, 256, num_layers=2, batch_first=True, dropout=0.25)\n",
              "  (encoder_out): Linear(in_features=256, out_features=100, bias=False)\n",
              "  (decoder_emb): Embedding(32653, 100, padding_idx=0)\n",
              "  (decoder_gru): GRU(100, 100, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (out_drop): Dropout(p=0.35)\n",
              "  (out): Linear(in_features=100, out_features=32653, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmL6vq-tODro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_anti_vocab(vocab):\n",
        "    res = dict()\n",
        "    for key in vocab:\n",
        "        res[vocab[key]] = key\n",
        "    return res\n",
        "\n",
        "def str_to_tensor(inp, vocab):\n",
        "    tokens = tokenize(preprocess(inp), vocab = vocab)\n",
        "    res = torch.zeros(len(tokens) + 2)\n",
        "    for i in range(1, len(tokens) + 1):\n",
        "        res[i] = vocab[tokens[i-1]]\n",
        "        \n",
        "    res[0] = vocab['xxxbos']\n",
        "    res[len(tokens) + 1] = vocab['xxxeos']\n",
        "    return res.unsqueeze(0).long()\n",
        "    \n",
        "def tensor_to_str(inp, anti_vocab):\n",
        "    res = []\n",
        "    for i in range(len(inp)):\n",
        "        res.append(anti_vocab[inp[i].item()])\n",
        "    return res\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbdJrA2Axq6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anti_vocab = generate_anti_vocab(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6f0Ov9exuE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = str_to_tensor(\"I am enjoying this year so far\", vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BwgKHZ_x1nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model(t.cuda(), vocab['xxxeos'], None, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zatX2pFQx5ps",
        "colab_type": "code",
        "outputId": "09a7dffd-d763-42c8-bf1d-56d4258fa780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred.shape, t.shape"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32653]), torch.Size([1, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t6GTIRzzbHo",
        "colab_type": "code",
        "outputId": "9cf52989-8c86-4525-8cd4-c708393eb41f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred.argmax(2).shape"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA7FWAin9rO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ea57500-c04d-4c8a-a150-8cdbce8d6350"
      },
      "source": [
        "tensor_to_str(pred.argmax(2).squeeze(0), anti_vocab)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxxbos', 'i', 'xxxeos']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM3p--dpOAwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}