{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewPochapsky/chatbot/blob/master/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0I5A7ULV5Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS74TvsTbjQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = Path('drive/My Drive/datasets/cornell movie-dialogs corpus')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRhpvb63aYdk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b82a71a6-c45c-4ff3-8692-6eb3e2620a99"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR2L2WOjZAVN",
        "colab_type": "text"
      },
      "source": [
        "# Training Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSrfFuwHaSgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line_map = {}\n",
        "with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        line_num = parts[0]\n",
        "        #-2 to get rid of \\n\n",
        "        text = parts[-1][:-2]\n",
        "        line_map[line_num] = text\n",
        "      \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4BHmalPi6JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = []\n",
        "with open(base_path/'movie_conversations.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        #get the referenced line numbers\n",
        "        line_nums = re.findall('L[0-9]+', parts[-1])\n",
        "        #form pairs\n",
        "        \n",
        "        for i in range(len(line_nums) - 1):\n",
        "            pair = (line_nums[i], line_nums[i+1])\n",
        "            #df.loc[df['column_name'] == some_value]\n",
        "            first = line_map[line_nums[i]]\n",
        "            second = line_map[line_nums[i+1]]\n",
        "            table.append([first, second])\n",
        "        \n",
        "            \n",
        "            \n",
        "data_df = pd.DataFrame(table, columns = ['in', 'out'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK6zOHYAjxtp",
        "colab_type": "code",
        "outputId": "ccee4e67-2a55-44d7-a5dc-8d8162f34473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>in</th>\n",
              "      <th>out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
              "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "      <td>Okay... then how 'bout we try out some French ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
              "      <td>Forget it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
              "      <td>Cameron</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  in                                                out\n",
              "0  Can we make this quick?  Roxanne Korrine and A...  Well, I thought we'd start with pronunciation,...\n",
              "1  Well, I thought we'd start with pronunciation,...  Not the hacking and gagging and spitting part....\n",
              "2  Not the hacking and gagging and spitting part....  Okay... then how 'bout we try out some French ...\n",
              "3  You're asking me out.  That's so cute. What's ...                                          Forget it\n",
              "4  No, no, it's my fault -- we didn't have a prop...                                            Cameron"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc78yc7jY3To",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTVhRWBJkSbX",
        "colab_type": "text"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthfvrbqY7hF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "595fed84-1be7-4ba2-be94-a55a556c8a72"
      },
      "source": [
        "def preprocess(s):\n",
        "    s = s.replace('\\n',' ').lower()\n",
        "    return s\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = spacy.blank(\"en\").tokenizer\n",
        "    doc = tokenizer(corpus)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if(token.text.strip() != \"\"):\n",
        "            tokens.append(token.text)\n",
        "    return tokens\n",
        "\n",
        "def process_dataset():\n",
        "    all_words = \"\"\n",
        "    with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            all_words += parts[-1]\n",
        "    return all_words\n",
        "\n",
        "def generate_vocab(tokens, min_freq = 0):\n",
        "    all_unique_words_counter = Counter(tokens)\n",
        "    vocab = {}\n",
        "    index = 0\n",
        "    for w in all_unique_words_counter.keys():\n",
        "        if(all_unique_words_counter[w] >= min_freq and w.strip() != \"\"):\n",
        "            vocab[w] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "#TODO: figure out how to only take words that appear at least x times\n",
        "def create_training_matrices(vocab, all_words, window_size = 5):\t\n",
        "\t\"\"\"\n",
        "        Returns x_train: Tensor()\n",
        "    \"\"\"\n",
        "\tnumTotalWords = len(all_words)\n",
        "\txTrain=[]\n",
        "\tyTrain=[]\n",
        "\tfor i in range(numTotalWords):\n",
        "\t\tif i % 100000 == 0:\n",
        "\t\t\tprint ('Finished %d/%d total words' % (i, numTotalWords))\n",
        "\t\twordsAfter = all_words[i + 1:i + window_size + 1]\n",
        "\t\twordsBefore = all_words[max(0, i - window_size):i]\n",
        "\t\twordsAdded = wordsAfter + wordsBefore\n",
        "\t\tfor word in wordsAdded:\n",
        "\t\t\txTrain.append(vocab[all_words[i]])\n",
        "\t\t\tyTrain.append(vocab[word])\n",
        "\treturn Tensor(xTrain), Tensor(yTrain)\n",
        "\n",
        "full_corpus = process_dataset() \n",
        "full_corpus = preprocess(full_corpus)\n",
        "print('Begin Tokenization')\n",
        "tokens = tokenize(full_corpus) # list of string\n",
        "print('Generating vocab')\n",
        "vocab = generate_vocab(tokens)\n",
        "print('Getting training data')\n",
        "x_train, y_train = create_training_matrices(vocab, tokens)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Tokenization\n",
            "Generating vocab\n",
            "Getting training data\n",
            "Finished 0/4194111 total words\n",
            "Finished 100000/4194111 total words\n",
            "Finished 200000/4194111 total words\n",
            "Finished 300000/4194111 total words\n",
            "Finished 400000/4194111 total words\n",
            "Finished 500000/4194111 total words\n",
            "Finished 600000/4194111 total words\n",
            "Finished 700000/4194111 total words\n",
            "Finished 800000/4194111 total words\n",
            "Finished 900000/4194111 total words\n",
            "Finished 1000000/4194111 total words\n",
            "Finished 1100000/4194111 total words\n",
            "Finished 1200000/4194111 total words\n",
            "Finished 1300000/4194111 total words\n",
            "Finished 1400000/4194111 total words\n",
            "Finished 1500000/4194111 total words\n",
            "Finished 1600000/4194111 total words\n",
            "Finished 1700000/4194111 total words\n",
            "Finished 1800000/4194111 total words\n",
            "Finished 1900000/4194111 total words\n",
            "Finished 2000000/4194111 total words\n",
            "Finished 2100000/4194111 total words\n",
            "Finished 2200000/4194111 total words\n",
            "Finished 2300000/4194111 total words\n",
            "Finished 2400000/4194111 total words\n",
            "Finished 2500000/4194111 total words\n",
            "Finished 2600000/4194111 total words\n",
            "Finished 2700000/4194111 total words\n",
            "Finished 2800000/4194111 total words\n",
            "Finished 2900000/4194111 total words\n",
            "Finished 3000000/4194111 total words\n",
            "Finished 3100000/4194111 total words\n",
            "Finished 3200000/4194111 total words\n",
            "Finished 3300000/4194111 total words\n",
            "Finished 3400000/4194111 total words\n",
            "Finished 3500000/4194111 total words\n",
            "Finished 3600000/4194111 total words\n",
            "Finished 3700000/4194111 total words\n",
            "Finished 3800000/4194111 total words\n",
            "Finished 3900000/4194111 total words\n",
            "Finished 4000000/4194111 total words\n",
            "Finished 4100000/4194111 total words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94YV0Yi-2rnJ",
        "colab_type": "text"
      },
      "source": [
        "SkipGram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeYxWCU2JBQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1daef4e1-7f7a-4f61-acad-3da8deb3a23a"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([41941080]), torch.Size([41941080]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avoAJTVmuGPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, emb_size, emb_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.center_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.context_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.init_emb()\n",
        "    \n",
        "    def init_emb(self):\n",
        "        initrange = 0.5 / self.emb_dim\n",
        "        self.center_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.context_embeddings.weight.data.uniform_(-0, 0)\n",
        "        \n",
        "    def forward(self, pos_center, pos_context, neg_context):\n",
        "        losses = []\n",
        "        emb_center = self.center_embeddings(pos_center)\n",
        "        emb_context = self.context_embeddings(pos_context)\n",
        "        score = torch.mul(emb_center, emb_context).squeeze()\n",
        "        print(score.shape)\n",
        "        score = torch.sum(score, dim = 1)\n",
        "        score = F.logsigmoid(score) # I think it is logsigmoid since we are doing nll loss func?\n",
        "        losses.append(sum(score))\n",
        "        \n",
        "        \n",
        "        neg_emb_context = self.context_embeddings(neg_context)\n",
        "        print(neg_emb_context.shape)\n",
        "        neg_score = torch.bmm(neg_emb_context, emb_center.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.sum(neg_score, dim = 1)\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        losses.append(sum(neg_score))\n",
        "        return -1 * sum(losses)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QZ94UCm_pfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size = 64, shuffle = True, num_workers = 4)\n",
        "\n",
        "def map_to_index(np_array, vocab):\n",
        "    output = torch.zeros(np_array.shape)\n",
        "    for i in range(len(np_array)):\n",
        "        output[i] = Tensor(list(map(lambda x: vocab[x], np_array[i])))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_JMxP2N2-al",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "outputId": "455beddf-801e-43c4-90cc-0011f3a3f1cf"
      },
      "source": [
        "lr = 3e-3\n",
        "num_epochs = 1\n",
        "neg_sample_size = 5\n",
        "emb_dim = 100\n",
        "emb_size = len(vocab.keys())\n",
        "model = SkipGramModel(emb_size, emb_dim).cuda()\n",
        "optim = torch.optim.SGD(model.parameters(), lr = lr) #cant use mom or wd since that would require calculating for all the params, too expensive\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        #neg sampling\n",
        "        neg_context = np.random.choice(\n",
        "            tokens,\n",
        "            size=(len(xb), neg_sample_size)\n",
        "        )\n",
        "        \n",
        "        neg_context = map_to_index(neg_context, vocab)\n",
        "        #print(type(neg_context))\n",
        "        loss = model(xb.cuda(), yb.cuda(), neg_context.cuda())\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    print('training_loss: ' + str(total_loss/len(xb)))\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-69f90cd24dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mneg_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(type(neg_context))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-4942811aba4a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pos_center, pos_context, neg_context)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mneg_emb_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_emb_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m         return F.embedding(\n\u001b[1;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got CUDAType instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWUnu9ydJW_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}