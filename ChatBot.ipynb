{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewPochapsky/chatbot/blob/master/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0I5A7ULV5Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from ast import literal_eval\n",
        "import spacy\n",
        "import pickle\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS74TvsTbjQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = Path('drive/My Drive/datasets/cornell movie-dialogs corpus')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRhpvb63aYdk",
        "colab_type": "code",
        "outputId": "e9205cb5-b4f1-4252-b663-0071f63ba157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc78yc7jY3To",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTVhRWBJkSbX",
        "colab_type": "text"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthfvrbqY7hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(s):\n",
        "    s = s.replace('\\n',' ').lower()\n",
        "    return s\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = spacy.blank(\"en\").tokenizer\n",
        "    doc = tokenizer(corpus)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if(token.text.strip() != \"\"):\n",
        "            tokens.append(token.text)\n",
        "    return tokens\n",
        "\n",
        "def process_dataset():\n",
        "    all_words = \"\"\n",
        "    with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            all_words += parts[-1]\n",
        "    return all_words\n",
        "\n",
        "def generate_vocab(tokens, min_freq = 0):\n",
        "    all_unique_words_counter = Counter(tokens)\n",
        "    vocab = {}\n",
        "    index = 0\n",
        "    for w in all_unique_words_counter.keys():\n",
        "        if(all_unique_words_counter[w] >= min_freq and w.strip() != \"\"):\n",
        "            vocab[w] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "def subsample(tokens, t = 1e-5):\n",
        "    \"\"\"\n",
        "        Paper: https://arxiv.org/pdf/1310.4546.pdf\n",
        "    \"\"\"\n",
        "    sampled_tokens = []\n",
        "    counter = Counter(tokens)\n",
        "    for token in tokens:\n",
        "        f_w = counter[token]/len(tokens)\n",
        "        p_w = 1 - math.sqrt(t/f_w)\n",
        "        val = random.uniform(0, 1)\n",
        "        if(val >= p_w):\n",
        "            sampled_tokens.append(token)\n",
        "            \n",
        "    return sampled_tokens\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "def create_training_matrices(vocab, all_words, window_size = 5):\t\n",
        "\t\"\"\"\n",
        "        Returns x_train: Tensor()\n",
        "    \"\"\"\n",
        "\tnumTotalWords = len(all_words)\n",
        "\txTrain=[]\n",
        "\tyTrain=[]\n",
        "\tfor i in range(numTotalWords):\n",
        "\t\twordsAfter = all_words[i + 1:i + window_size + 1]\n",
        "\t\twordsBefore = all_words[max(0, i - window_size):i]\n",
        "\t\twordsAdded = wordsAfter + wordsBefore\n",
        "\t\tfor word in wordsAdded:\n",
        "\t\t\txTrain.append(vocab[all_words[i]])\n",
        "\t\t\tyTrain.append(vocab[word])\n",
        "\treturn Tensor(xTrain), Tensor(yTrain)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUKWTcYDu9_j",
        "colab_type": "code",
        "outputId": "5ebbcbe4-eaaf-4361-dd25-8cc14d6a9337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "\n",
        "full_corpus = process_dataset() \n",
        "full_corpus = preprocess(full_corpus)\n",
        "print('Begin Tokenization')\n",
        "tokens = tokenize(full_corpus) # list of string\n",
        "print('Generating vocab')\n",
        "vocab = generate_vocab(tokens)\n",
        "print(len(tokens))\n",
        "print('Subsampling data')\n",
        "tokens = subsample(tokens, 1e-5)\n",
        "print(len(tokens))\n",
        "print('Getting training data')\n",
        "x_train, y_train = create_training_matrices(vocab, tokens, window_size = 3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Tokenization\n",
            "Generating vocab\n",
            "4194111\n",
            "Subsampling data\n",
            "688935\n",
            "Getting training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94YV0Yi-2rnJ",
        "colab_type": "text"
      },
      "source": [
        "SkipGram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avoAJTVmuGPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, emb_size, emb_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.center_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.context_embeddings = nn.Embedding(emb_size, emb_dim, sparse = True)\n",
        "        self.init_emb()\n",
        "    \n",
        "    def init_emb(self):\n",
        "        #initrange = 0.5 / self.emb_dim\n",
        "        #self.center_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        nn.init.kaiming_uniform_(self.center_embeddings.weight, a=math.sqrt(5))\n",
        "        self.context_embeddings.weight.data.uniform_(-0, 0)\n",
        "        \n",
        "    def forward(self, pos_center, pos_context, neg_context):\n",
        "        losses = []\n",
        "        emb_center = self.center_embeddings(pos_center.long())\n",
        "        emb_context = self.context_embeddings(pos_context.long())\n",
        "        score = torch.mul(emb_center, emb_context).squeeze()\n",
        "        #print(score.shape)\n",
        "        score = torch.sum(score, dim = 1)\n",
        "        score = F.logsigmoid(score) # I think it is logsigmoid since we are doing nll loss func?\n",
        "        losses.append(sum(score))\n",
        "        \n",
        "        \n",
        "        neg_emb_context = self.context_embeddings(neg_context.long())\n",
        "        #print(neg_emb_context.shape)\n",
        "        neg_score = torch.bmm(neg_emb_context, emb_center.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.sum(neg_score, dim = 1)\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        losses.append(sum(neg_score))\n",
        "        return -1 * sum(losses)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QZ94UCm_pfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size = 128, shuffle = False, num_workers = 4)\n",
        "\n",
        "def map_to_index(np_array, vocab):\n",
        "    output = torch.zeros(np_array.shape)\n",
        "    for i in range(len(np_array)):\n",
        "        output[i] = Tensor(list(map(lambda x: vocab[x], np_array[i])))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_JMxP2N2-al",
        "colab_type": "code",
        "outputId": "3e8e09d4-35b8-4499-9258-c77caec4cac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr = 3e-3\n",
        "\n",
        "num_epochs = 1\n",
        "neg_sample_size = 5\n",
        "emb_dim = 100\n",
        "emb_size = len(vocab.keys())\n",
        "model = SkipGramModel(emb_size, emb_dim)\n",
        "optim = torch.optim.SGD(model.parameters(), lr = lr) #cant use mom or wd since that would require calculating for all the params, too expensive\n",
        "i = 0\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        #neg sampling\n",
        "        neg_context = np.random.choice(\n",
        "            tokens,\n",
        "            size=(len(xb), neg_sample_size)\n",
        "        )\n",
        "        \n",
        "        neg_context = map_to_index(neg_context, vocab)\n",
        "        #print(type(neg_context))\n",
        "        optim.zero_grad()\n",
        "        loss = model(xb, yb, neg_context)\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if(i % 100 == 0):\n",
        "            print('completed {0}/{1} batches. Avg loss per batch: {2}'.format(i, len(train_dl), total_loss/(i)))\n",
        "        i+=1\n",
        "    \n",
        "    \n",
        "torch.save(model.state_dict(), base_path/'word2vec.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed 0/32296 batches. Avg loss per batch: inf\n",
            "completed 100/32296 batches. Avg loss per batch: 179.21861267089844\n",
            "completed 200/32296 batches. Avg loss per batch: 178.33065795898438\n",
            "completed 300/32296 batches. Avg loss per batch: 178.03488159179688\n",
            "completed 400/32296 batches. Avg loss per batch: 177.8870849609375\n",
            "completed 500/32296 batches. Avg loss per batch: 177.79783630371094\n",
            "completed 600/32296 batches. Avg loss per batch: 177.73805236816406\n",
            "completed 700/32296 batches. Avg loss per batch: 177.69508361816406\n",
            "completed 800/32296 batches. Avg loss per batch: 177.66226196289062\n",
            "completed 900/32296 batches. Avg loss per batch: 177.63685607910156\n",
            "completed 1000/32296 batches. Avg loss per batch: 177.61656188964844\n",
            "completed 1100/32296 batches. Avg loss per batch: 177.60043334960938\n",
            "completed 1200/32296 batches. Avg loss per batch: 177.5867156982422\n",
            "completed 1300/32296 batches. Avg loss per batch: 177.57479858398438\n",
            "completed 1400/32296 batches. Avg loss per batch: 177.56460571289062\n",
            "completed 1500/32296 batches. Avg loss per batch: 177.55564880371094\n",
            "completed 1600/32296 batches. Avg loss per batch: 177.54757690429688\n",
            "completed 1700/32296 batches. Avg loss per batch: 177.54025268554688\n",
            "completed 1800/32296 batches. Avg loss per batch: 177.53427124023438\n",
            "completed 1900/32296 batches. Avg loss per batch: 177.5281982421875\n",
            "completed 2000/32296 batches. Avg loss per batch: 177.52328491210938\n",
            "completed 2100/32296 batches. Avg loss per batch: 177.5183868408203\n",
            "completed 2200/32296 batches. Avg loss per batch: 177.51388549804688\n",
            "completed 2300/32296 batches. Avg loss per batch: 177.50950622558594\n",
            "completed 2400/32296 batches. Avg loss per batch: 177.50572204589844\n",
            "completed 2500/32296 batches. Avg loss per batch: 177.50209045410156\n",
            "completed 2600/32296 batches. Avg loss per batch: 177.4989471435547\n",
            "completed 2700/32296 batches. Avg loss per batch: 177.495849609375\n",
            "completed 2800/32296 batches. Avg loss per batch: 177.49270629882812\n",
            "completed 2900/32296 batches. Avg loss per batch: 177.48976135253906\n",
            "completed 3000/32296 batches. Avg loss per batch: 177.48727416992188\n",
            "completed 3100/32296 batches. Avg loss per batch: 177.48451232910156\n",
            "completed 3200/32296 batches. Avg loss per batch: 177.48178100585938\n",
            "completed 3300/32296 batches. Avg loss per batch: 177.4789581298828\n",
            "completed 3400/32296 batches. Avg loss per batch: 177.47657775878906\n",
            "completed 3500/32296 batches. Avg loss per batch: 177.4739990234375\n",
            "completed 3600/32296 batches. Avg loss per batch: 177.4720458984375\n",
            "completed 3700/32296 batches. Avg loss per batch: 177.4696044921875\n",
            "completed 3800/32296 batches. Avg loss per batch: 177.4674072265625\n",
            "completed 3900/32296 batches. Avg loss per batch: 177.46539306640625\n",
            "completed 4000/32296 batches. Avg loss per batch: 177.46310424804688\n",
            "completed 4100/32296 batches. Avg loss per batch: 177.460693359375\n",
            "completed 4200/32296 batches. Avg loss per batch: 177.4582977294922\n",
            "completed 4300/32296 batches. Avg loss per batch: 177.45632934570312\n",
            "completed 4400/32296 batches. Avg loss per batch: 177.45419311523438\n",
            "completed 4500/32296 batches. Avg loss per batch: 177.4518585205078\n",
            "completed 4600/32296 batches. Avg loss per batch: 177.4499053955078\n",
            "completed 4700/32296 batches. Avg loss per batch: 177.4474334716797\n",
            "completed 4800/32296 batches. Avg loss per batch: 177.44517517089844\n",
            "completed 4900/32296 batches. Avg loss per batch: 177.44285583496094\n",
            "completed 5000/32296 batches. Avg loss per batch: 177.44015502929688\n",
            "completed 5100/32296 batches. Avg loss per batch: 177.4371795654297\n",
            "completed 5200/32296 batches. Avg loss per batch: 177.43482971191406\n",
            "completed 5300/32296 batches. Avg loss per batch: 177.4319305419922\n",
            "completed 5400/32296 batches. Avg loss per batch: 177.42942810058594\n",
            "completed 5500/32296 batches. Avg loss per batch: 177.42710876464844\n",
            "completed 5600/32296 batches. Avg loss per batch: 177.4251708984375\n",
            "completed 5700/32296 batches. Avg loss per batch: 177.42259216308594\n",
            "completed 5800/32296 batches. Avg loss per batch: 177.41954040527344\n",
            "completed 5900/32296 batches. Avg loss per batch: 177.41702270507812\n",
            "completed 6000/32296 batches. Avg loss per batch: 177.41456604003906\n",
            "completed 6100/32296 batches. Avg loss per batch: 177.41114807128906\n",
            "completed 6200/32296 batches. Avg loss per batch: 177.4076385498047\n",
            "completed 6300/32296 batches. Avg loss per batch: 177.40354919433594\n",
            "completed 6400/32296 batches. Avg loss per batch: 177.40135192871094\n",
            "completed 6500/32296 batches. Avg loss per batch: 177.39768981933594\n",
            "completed 6600/32296 batches. Avg loss per batch: 177.39382934570312\n",
            "completed 6700/32296 batches. Avg loss per batch: 177.38941955566406\n",
            "completed 6800/32296 batches. Avg loss per batch: 177.38592529296875\n",
            "completed 6900/32296 batches. Avg loss per batch: 177.3817596435547\n",
            "completed 7000/32296 batches. Avg loss per batch: 177.37689208984375\n",
            "completed 7100/32296 batches. Avg loss per batch: 177.37234497070312\n",
            "completed 7200/32296 batches. Avg loss per batch: 177.3681182861328\n",
            "completed 7300/32296 batches. Avg loss per batch: 177.3639373779297\n",
            "completed 7400/32296 batches. Avg loss per batch: 177.35984802246094\n",
            "completed 7500/32296 batches. Avg loss per batch: 177.3554229736328\n",
            "completed 7600/32296 batches. Avg loss per batch: 177.351318359375\n",
            "completed 7700/32296 batches. Avg loss per batch: 177.34617614746094\n",
            "completed 7800/32296 batches. Avg loss per batch: 177.3407440185547\n",
            "completed 7900/32296 batches. Avg loss per batch: 177.33607482910156\n",
            "completed 8000/32296 batches. Avg loss per batch: 177.33047485351562\n",
            "completed 8100/32296 batches. Avg loss per batch: 177.32522583007812\n",
            "completed 8200/32296 batches. Avg loss per batch: 177.3192901611328\n",
            "completed 8300/32296 batches. Avg loss per batch: 177.3136444091797\n",
            "completed 8400/32296 batches. Avg loss per batch: 177.3066864013672\n",
            "completed 8500/32296 batches. Avg loss per batch: 177.29974365234375\n",
            "completed 8600/32296 batches. Avg loss per batch: 177.29283142089844\n",
            "completed 8700/32296 batches. Avg loss per batch: 177.285400390625\n",
            "completed 8800/32296 batches. Avg loss per batch: 177.27874755859375\n",
            "completed 8900/32296 batches. Avg loss per batch: 177.2714385986328\n",
            "completed 9000/32296 batches. Avg loss per batch: 177.2624053955078\n",
            "completed 9100/32296 batches. Avg loss per batch: 177.25474548339844\n",
            "completed 9200/32296 batches. Avg loss per batch: 177.2476348876953\n",
            "completed 9300/32296 batches. Avg loss per batch: 177.23834228515625\n",
            "completed 9400/32296 batches. Avg loss per batch: 177.2281036376953\n",
            "completed 9500/32296 batches. Avg loss per batch: 177.21730041503906\n",
            "completed 9600/32296 batches. Avg loss per batch: 177.2070770263672\n",
            "completed 9700/32296 batches. Avg loss per batch: 177.19595336914062\n",
            "completed 9800/32296 batches. Avg loss per batch: 177.18531799316406\n",
            "completed 9900/32296 batches. Avg loss per batch: 177.172607421875\n",
            "completed 10000/32296 batches. Avg loss per batch: 177.1602783203125\n",
            "completed 10100/32296 batches. Avg loss per batch: 177.150146484375\n",
            "completed 10200/32296 batches. Avg loss per batch: 177.13819885253906\n",
            "completed 10300/32296 batches. Avg loss per batch: 177.12782287597656\n",
            "completed 10400/32296 batches. Avg loss per batch: 177.11813354492188\n",
            "completed 10500/32296 batches. Avg loss per batch: 177.1077117919922\n",
            "completed 10600/32296 batches. Avg loss per batch: 177.09698486328125\n",
            "completed 10700/32296 batches. Avg loss per batch: 177.08560180664062\n",
            "completed 10800/32296 batches. Avg loss per batch: 177.0728302001953\n",
            "completed 10900/32296 batches. Avg loss per batch: 177.05938720703125\n",
            "completed 11000/32296 batches. Avg loss per batch: 177.04568481445312\n",
            "completed 11100/32296 batches. Avg loss per batch: 177.03041076660156\n",
            "completed 11200/32296 batches. Avg loss per batch: 177.0143280029297\n",
            "completed 11300/32296 batches. Avg loss per batch: 176.99801635742188\n",
            "completed 11400/32296 batches. Avg loss per batch: 176.97991943359375\n",
            "completed 11500/32296 batches. Avg loss per batch: 176.9634552001953\n",
            "completed 11600/32296 batches. Avg loss per batch: 176.94422912597656\n",
            "completed 11700/32296 batches. Avg loss per batch: 176.9259033203125\n",
            "completed 11800/32296 batches. Avg loss per batch: 176.9047088623047\n",
            "completed 11900/32296 batches. Avg loss per batch: 176.88714599609375\n",
            "completed 12000/32296 batches. Avg loss per batch: 176.8689422607422\n",
            "completed 12100/32296 batches. Avg loss per batch: 176.8502960205078\n",
            "completed 12200/32296 batches. Avg loss per batch: 176.83229064941406\n",
            "completed 12300/32296 batches. Avg loss per batch: 176.8115692138672\n",
            "completed 12400/32296 batches. Avg loss per batch: 176.78909301757812\n",
            "completed 12500/32296 batches. Avg loss per batch: 176.76832580566406\n",
            "completed 12600/32296 batches. Avg loss per batch: 176.74588012695312\n",
            "completed 12700/32296 batches. Avg loss per batch: 176.72447204589844\n",
            "completed 12800/32296 batches. Avg loss per batch: 176.69837951660156\n",
            "completed 12900/32296 batches. Avg loss per batch: 176.67587280273438\n",
            "completed 13000/32296 batches. Avg loss per batch: 176.65419006347656\n",
            "completed 13100/32296 batches. Avg loss per batch: 176.63002014160156\n",
            "completed 13200/32296 batches. Avg loss per batch: 176.60928344726562\n",
            "completed 13300/32296 batches. Avg loss per batch: 176.5860137939453\n",
            "completed 13400/32296 batches. Avg loss per batch: 176.55990600585938\n",
            "completed 13500/32296 batches. Avg loss per batch: 176.53314208984375\n",
            "completed 13600/32296 batches. Avg loss per batch: 176.50701904296875\n",
            "completed 13700/32296 batches. Avg loss per batch: 176.4775848388672\n",
            "completed 13800/32296 batches. Avg loss per batch: 176.45115661621094\n",
            "completed 13900/32296 batches. Avg loss per batch: 176.42271423339844\n",
            "completed 14000/32296 batches. Avg loss per batch: 176.39378356933594\n",
            "completed 14100/32296 batches. Avg loss per batch: 176.36441040039062\n",
            "completed 14200/32296 batches. Avg loss per batch: 176.33560180664062\n",
            "completed 14300/32296 batches. Avg loss per batch: 176.31163024902344\n",
            "completed 14400/32296 batches. Avg loss per batch: 176.2806396484375\n",
            "completed 14500/32296 batches. Avg loss per batch: 176.2506561279297\n",
            "completed 14600/32296 batches. Avg loss per batch: 176.21807861328125\n",
            "completed 14700/32296 batches. Avg loss per batch: 176.1864013671875\n",
            "completed 14800/32296 batches. Avg loss per batch: 176.15786743164062\n",
            "completed 14900/32296 batches. Avg loss per batch: 176.13125610351562\n",
            "completed 15000/32296 batches. Avg loss per batch: 176.0972442626953\n",
            "completed 15100/32296 batches. Avg loss per batch: 176.066162109375\n",
            "completed 15200/32296 batches. Avg loss per batch: 176.03172302246094\n",
            "completed 15300/32296 batches. Avg loss per batch: 175.99380493164062\n",
            "completed 15400/32296 batches. Avg loss per batch: 175.96092224121094\n",
            "completed 15500/32296 batches. Avg loss per batch: 175.9241943359375\n",
            "completed 15600/32296 batches. Avg loss per batch: 175.8908233642578\n",
            "completed 15700/32296 batches. Avg loss per batch: 175.85491943359375\n",
            "completed 15800/32296 batches. Avg loss per batch: 175.8166046142578\n",
            "completed 15900/32296 batches. Avg loss per batch: 175.7813262939453\n",
            "completed 16000/32296 batches. Avg loss per batch: 175.74424743652344\n",
            "completed 16100/32296 batches. Avg loss per batch: 175.70672607421875\n",
            "completed 16200/32296 batches. Avg loss per batch: 175.668701171875\n",
            "completed 16300/32296 batches. Avg loss per batch: 175.63038635253906\n",
            "completed 16400/32296 batches. Avg loss per batch: 175.59202575683594\n",
            "completed 16500/32296 batches. Avg loss per batch: 175.55389404296875\n",
            "completed 16600/32296 batches. Avg loss per batch: 175.52142333984375\n",
            "completed 16700/32296 batches. Avg loss per batch: 175.48594665527344\n",
            "completed 16800/32296 batches. Avg loss per batch: 175.4498291015625\n",
            "completed 16900/32296 batches. Avg loss per batch: 175.41737365722656\n",
            "completed 17000/32296 batches. Avg loss per batch: 175.3828125\n",
            "completed 17100/32296 batches. Avg loss per batch: 175.34410095214844\n",
            "completed 17200/32296 batches. Avg loss per batch: 175.302978515625\n",
            "completed 17300/32296 batches. Avg loss per batch: 175.25958251953125\n",
            "completed 17400/32296 batches. Avg loss per batch: 175.22325134277344\n",
            "completed 17500/32296 batches. Avg loss per batch: 175.1824951171875\n",
            "completed 17600/32296 batches. Avg loss per batch: 175.14089965820312\n",
            "completed 17700/32296 batches. Avg loss per batch: 175.10134887695312\n",
            "completed 17800/32296 batches. Avg loss per batch: 175.0620574951172\n",
            "completed 17900/32296 batches. Avg loss per batch: 175.02532958984375\n",
            "completed 18000/32296 batches. Avg loss per batch: 174.9835205078125\n",
            "completed 18100/32296 batches. Avg loss per batch: 174.94297790527344\n",
            "completed 18200/32296 batches. Avg loss per batch: 174.90476989746094\n",
            "completed 18300/32296 batches. Avg loss per batch: 174.86566162109375\n",
            "completed 18400/32296 batches. Avg loss per batch: 174.82379150390625\n",
            "completed 18500/32296 batches. Avg loss per batch: 174.78152465820312\n",
            "completed 18600/32296 batches. Avg loss per batch: 174.74305725097656\n",
            "completed 18700/32296 batches. Avg loss per batch: 174.7016143798828\n",
            "completed 18800/32296 batches. Avg loss per batch: 174.65936279296875\n",
            "completed 18900/32296 batches. Avg loss per batch: 174.616943359375\n",
            "completed 19000/32296 batches. Avg loss per batch: 174.57518005371094\n",
            "completed 19100/32296 batches. Avg loss per batch: 174.5337371826172\n",
            "completed 19200/32296 batches. Avg loss per batch: 174.4939727783203\n",
            "completed 19300/32296 batches. Avg loss per batch: 174.45013427734375\n",
            "completed 19400/32296 batches. Avg loss per batch: 174.4131317138672\n",
            "completed 19500/32296 batches. Avg loss per batch: 174.36788940429688\n",
            "completed 19600/32296 batches. Avg loss per batch: 174.32363891601562\n",
            "completed 19700/32296 batches. Avg loss per batch: 174.28038024902344\n",
            "completed 19800/32296 batches. Avg loss per batch: 174.23512268066406\n",
            "completed 19900/32296 batches. Avg loss per batch: 174.1872100830078\n",
            "completed 20000/32296 batches. Avg loss per batch: 174.143310546875\n",
            "completed 20100/32296 batches. Avg loss per batch: 174.0975799560547\n",
            "completed 20200/32296 batches. Avg loss per batch: 174.0561981201172\n",
            "completed 20300/32296 batches. Avg loss per batch: 174.01295471191406\n",
            "completed 20400/32296 batches. Avg loss per batch: 173.9681396484375\n",
            "completed 20500/32296 batches. Avg loss per batch: 173.92355346679688\n",
            "completed 20600/32296 batches. Avg loss per batch: 173.8814239501953\n",
            "completed 20700/32296 batches. Avg loss per batch: 173.83627319335938\n",
            "completed 20800/32296 batches. Avg loss per batch: 173.7918701171875\n",
            "completed 20900/32296 batches. Avg loss per batch: 173.7439422607422\n",
            "completed 21000/32296 batches. Avg loss per batch: 173.6965789794922\n",
            "completed 21100/32296 batches. Avg loss per batch: 173.65164184570312\n",
            "completed 21200/32296 batches. Avg loss per batch: 173.60887145996094\n",
            "completed 21300/32296 batches. Avg loss per batch: 173.56365966796875\n",
            "completed 21400/32296 batches. Avg loss per batch: 173.5165557861328\n",
            "completed 21500/32296 batches. Avg loss per batch: 173.471923828125\n",
            "completed 21600/32296 batches. Avg loss per batch: 173.4242706298828\n",
            "completed 21700/32296 batches. Avg loss per batch: 173.37454223632812\n",
            "completed 21800/32296 batches. Avg loss per batch: 173.3260498046875\n",
            "completed 21900/32296 batches. Avg loss per batch: 173.2828826904297\n",
            "completed 22000/32296 batches. Avg loss per batch: 173.23875427246094\n",
            "completed 22100/32296 batches. Avg loss per batch: 173.1907196044922\n",
            "completed 22200/32296 batches. Avg loss per batch: 173.14405822753906\n",
            "completed 22300/32296 batches. Avg loss per batch: 173.09768676757812\n",
            "completed 22400/32296 batches. Avg loss per batch: 173.05552673339844\n",
            "completed 22500/32296 batches. Avg loss per batch: 173.01004028320312\n",
            "completed 22600/32296 batches. Avg loss per batch: 172.96519470214844\n",
            "completed 22700/32296 batches. Avg loss per batch: 172.91831970214844\n",
            "completed 22800/32296 batches. Avg loss per batch: 172.87144470214844\n",
            "completed 22900/32296 batches. Avg loss per batch: 172.82383728027344\n",
            "completed 23000/32296 batches. Avg loss per batch: 172.781005859375\n",
            "completed 23100/32296 batches. Avg loss per batch: 172.733642578125\n",
            "completed 23200/32296 batches. Avg loss per batch: 172.68812561035156\n",
            "completed 23300/32296 batches. Avg loss per batch: 172.6439971923828\n",
            "completed 23400/32296 batches. Avg loss per batch: 172.59722900390625\n",
            "completed 23500/32296 batches. Avg loss per batch: 172.55111694335938\n",
            "completed 23600/32296 batches. Avg loss per batch: 172.50440979003906\n",
            "completed 23700/32296 batches. Avg loss per batch: 172.4587860107422\n",
            "completed 23800/32296 batches. Avg loss per batch: 172.41232299804688\n",
            "completed 23900/32296 batches. Avg loss per batch: 172.36196899414062\n",
            "completed 24000/32296 batches. Avg loss per batch: 172.315673828125\n",
            "completed 24100/32296 batches. Avg loss per batch: 172.26942443847656\n",
            "completed 24200/32296 batches. Avg loss per batch: 172.2232666015625\n",
            "completed 24300/32296 batches. Avg loss per batch: 172.17662048339844\n",
            "completed 24400/32296 batches. Avg loss per batch: 172.1270751953125\n",
            "completed 24500/32296 batches. Avg loss per batch: 172.0819549560547\n",
            "completed 24600/32296 batches. Avg loss per batch: 172.03341674804688\n",
            "completed 24700/32296 batches. Avg loss per batch: 171.98486328125\n",
            "completed 24800/32296 batches. Avg loss per batch: 171.93731689453125\n",
            "completed 24900/32296 batches. Avg loss per batch: 171.89004516601562\n",
            "completed 25000/32296 batches. Avg loss per batch: 171.84463500976562\n",
            "completed 25100/32296 batches. Avg loss per batch: 171.79507446289062\n",
            "completed 25200/32296 batches. Avg loss per batch: 171.74862670898438\n",
            "completed 25300/32296 batches. Avg loss per batch: 171.69998168945312\n",
            "completed 25400/32296 batches. Avg loss per batch: 171.6554718017578\n",
            "completed 25500/32296 batches. Avg loss per batch: 171.60748291015625\n",
            "completed 25600/32296 batches. Avg loss per batch: 171.5580291748047\n",
            "completed 25700/32296 batches. Avg loss per batch: 171.51055908203125\n",
            "completed 25800/32296 batches. Avg loss per batch: 171.46104431152344\n",
            "completed 25900/32296 batches. Avg loss per batch: 171.41714477539062\n",
            "completed 26000/32296 batches. Avg loss per batch: 171.370849609375\n",
            "completed 26100/32296 batches. Avg loss per batch: 171.32009887695312\n",
            "completed 26200/32296 batches. Avg loss per batch: 171.27427673339844\n",
            "completed 26300/32296 batches. Avg loss per batch: 171.22735595703125\n",
            "completed 26400/32296 batches. Avg loss per batch: 171.1781463623047\n",
            "completed 26500/32296 batches. Avg loss per batch: 171.13241577148438\n",
            "completed 26600/32296 batches. Avg loss per batch: 171.0811767578125\n",
            "completed 26700/32296 batches. Avg loss per batch: 171.03375244140625\n",
            "completed 26800/32296 batches. Avg loss per batch: 170.98907470703125\n",
            "completed 26900/32296 batches. Avg loss per batch: 170.9432830810547\n",
            "completed 27000/32296 batches. Avg loss per batch: 170.89581298828125\n",
            "completed 27100/32296 batches. Avg loss per batch: 170.84727478027344\n",
            "completed 27200/32296 batches. Avg loss per batch: 170.79811096191406\n",
            "completed 27300/32296 batches. Avg loss per batch: 170.7505645751953\n",
            "completed 27400/32296 batches. Avg loss per batch: 170.7059326171875\n",
            "completed 27500/32296 batches. Avg loss per batch: 170.65975952148438\n",
            "completed 27600/32296 batches. Avg loss per batch: 170.6128387451172\n",
            "completed 27700/32296 batches. Avg loss per batch: 170.56483459472656\n",
            "completed 27800/32296 batches. Avg loss per batch: 170.5187225341797\n",
            "completed 27900/32296 batches. Avg loss per batch: 170.47520446777344\n",
            "completed 28000/32296 batches. Avg loss per batch: 170.43214416503906\n",
            "completed 28100/32296 batches. Avg loss per batch: 170.38800048828125\n",
            "completed 28200/32296 batches. Avg loss per batch: 170.34332275390625\n",
            "completed 28300/32296 batches. Avg loss per batch: 170.2965087890625\n",
            "completed 28400/32296 batches. Avg loss per batch: 170.24945068359375\n",
            "completed 28500/32296 batches. Avg loss per batch: 170.20468139648438\n",
            "completed 28600/32296 batches. Avg loss per batch: 170.15859985351562\n",
            "completed 28700/32296 batches. Avg loss per batch: 170.10928344726562\n",
            "completed 28800/32296 batches. Avg loss per batch: 170.0609130859375\n",
            "completed 28900/32296 batches. Avg loss per batch: 170.01806640625\n",
            "completed 29000/32296 batches. Avg loss per batch: 169.97401428222656\n",
            "completed 29100/32296 batches. Avg loss per batch: 169.9287109375\n",
            "completed 29200/32296 batches. Avg loss per batch: 169.8839111328125\n",
            "completed 29300/32296 batches. Avg loss per batch: 169.83770751953125\n",
            "completed 29400/32296 batches. Avg loss per batch: 169.7979278564453\n",
            "completed 29500/32296 batches. Avg loss per batch: 169.753662109375\n",
            "completed 29600/32296 batches. Avg loss per batch: 169.7064208984375\n",
            "completed 29700/32296 batches. Avg loss per batch: 169.65951538085938\n",
            "completed 29800/32296 batches. Avg loss per batch: 169.61322021484375\n",
            "completed 29900/32296 batches. Avg loss per batch: 169.56871032714844\n",
            "completed 30000/32296 batches. Avg loss per batch: 169.526123046875\n",
            "completed 30100/32296 batches. Avg loss per batch: 169.4844970703125\n",
            "completed 30200/32296 batches. Avg loss per batch: 169.44000244140625\n",
            "completed 30300/32296 batches. Avg loss per batch: 169.39617919921875\n",
            "completed 30400/32296 batches. Avg loss per batch: 169.34910583496094\n",
            "completed 30500/32296 batches. Avg loss per batch: 169.30223083496094\n",
            "completed 30600/32296 batches. Avg loss per batch: 169.25689697265625\n",
            "completed 30700/32296 batches. Avg loss per batch: 169.21237182617188\n",
            "completed 30800/32296 batches. Avg loss per batch: 169.16551208496094\n",
            "completed 30900/32296 batches. Avg loss per batch: 169.12161254882812\n",
            "completed 31000/32296 batches. Avg loss per batch: 169.07400512695312\n",
            "completed 31100/32296 batches. Avg loss per batch: 169.02734375\n",
            "completed 31200/32296 batches. Avg loss per batch: 168.9846954345703\n",
            "completed 31300/32296 batches. Avg loss per batch: 168.9423828125\n",
            "completed 31400/32296 batches. Avg loss per batch: 168.89666748046875\n",
            "completed 31500/32296 batches. Avg loss per batch: 168.8552703857422\n",
            "completed 31600/32296 batches. Avg loss per batch: 168.81158447265625\n",
            "completed 31700/32296 batches. Avg loss per batch: 168.7676239013672\n",
            "completed 31800/32296 batches. Avg loss per batch: 168.7263946533203\n",
            "completed 31900/32296 batches. Avg loss per batch: 168.68240356445312\n",
            "completed 32000/32296 batches. Avg loss per batch: 168.64065551757812\n",
            "completed 32100/32296 batches. Avg loss per batch: 168.5970458984375\n",
            "completed 32200/32296 batches. Avg loss per batch: 168.55319213867188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFZBp1wWtt8U",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcMUbFbcGTea",
        "colab_type": "text"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukHUcLxPGQdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line_map = {}\n",
        "with open(base_path/'movie_lines.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        line_num = parts[0]\n",
        "        #-2 to get rid of \\n\n",
        "        text = parts[-1][:-1]\n",
        "        line_map[line_num] = text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0opgwHBGRGH",
        "colab_type": "code",
        "outputId": "5a0f0d59-e491-4dec-e1a9-e26f4d87ef8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "table = []\n",
        "with open(base_path/'movie_conversations.txt', encoding = 'ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.split(' +++$+++ ')\n",
        "        #get the referenced line numbers\n",
        "        line_nums = re.findall('L[0-9]+', parts[-1])\n",
        "        #form pairs\n",
        "        \n",
        "        for i in range(len(line_nums) - 1):\n",
        "            pair = (line_nums[i], line_nums[i+1])\n",
        "            #df.loc[df['column_name'] == some_value]\n",
        "            first = line_map[line_nums[i]]\n",
        "            second = line_map[line_nums[i+1]]\n",
        "            table.append([tokenize(preprocess(first)), tokenize(preprocess(second))])\n",
        "        \n",
        "data_df = pd.DataFrame(table, columns = ['in', 'out'])\n",
        "data_df.to_csv(base_path/'processed_data.csv', index = False)\n",
        "data_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>in</th>\n",
              "      <th>out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[can, we, make, this, quick, ?, roxanne, korri...</td>\n",
              "      <td>[well, ,, i, thought, we, 'd, start, with, pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[well, ,, i, thought, we, 'd, start, with, pro...</td>\n",
              "      <td>[not, the, hacking, and, gagging, and, spittin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[not, the, hacking, and, gagging, and, spittin...</td>\n",
              "      <td>[okay, ..., then, how, 'bout, we, try, out, so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[you, 're, asking, me, out, ., that, 's, so, c...</td>\n",
              "      <td>[forget, it, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[no, ,, no, ,, it, 's, my, fault, --, we, did,...</td>\n",
              "      <td>[cameron, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  in                                                out\n",
              "0  [can, we, make, this, quick, ?, roxanne, korri...  [well, ,, i, thought, we, 'd, start, with, pro...\n",
              "1  [well, ,, i, thought, we, 'd, start, with, pro...  [not, the, hacking, and, gagging, and, spittin...\n",
              "2  [not, the, hacking, and, gagging, and, spittin...  [okay, ..., then, how, 'bout, we, try, out, so...\n",
              "3  [you, 're, asking, me, out, ., that, 's, so, c...                                    [forget, it, .]\n",
              "4  [no, ,, no, ,, it, 's, my, fault, --, we, did,...                                       [cameron, .]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFiis_fjY2b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def converter(x):\n",
        "    #convert \"list\" to list\n",
        "    return literal_eval(x)\n",
        "\n",
        "converters={'in': converter, 'out': converter}\n",
        "df = pd.read_csv(base_path/'processed_data.csv', converters = converters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Dj7BZNGYdw",
        "colab_type": "text"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ocnidotyGS",
        "colab_type": "code",
        "outputId": "759b6099-5f34-4e0c-ee87-46b2c1dba071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "emb_dim = 100\n",
        "emb_size = len(vocab.keys())\n",
        "word2vec_model = SkipGramModel(emb_size, emb_dim)\n",
        "word2vec_model.load_state_dict(torch.load(base_path/'word2vec.pt'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ifkijpHmkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add special tokens to the vocab\n",
        "vocab['xxxpad'] = emb_size\n",
        "emb_size += 1\n",
        "vocab['xxxeos'] = emb_size\n",
        "emb_size += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hE7FEaYIMBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(word2vec_model, emb_size, emb_dim, padding_idx, eos_idx):\n",
        "    new_emb = nn.Embedding(emb_size, emb_dim, padding_idx=padding_idx)\n",
        "    old_emb_weights = word2vec_model.center_embeddings.weight.data\n",
        "    for i in range(len(old_emb_weights)):\n",
        "        new_emb.weight.data[i] = old_emb_weights[i]\n",
        "    \n",
        "    #init the new embeddings to zero\n",
        "    new_emb.weight.data[padding_idx].uniform_(-old_emb_weights.mean(), old_emb_weights.mean())\n",
        "    new_emb.weight.data[eos_idx].uniform_(-old_emb_weights.mean(), old_emb_weights.mean())\n",
        "    \n",
        "    return new_emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v2JG88euzjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb = create_embedding_matrix(word2vec_model, emb_size, emb_dim, vocab['xxxpad'], vocab['xxxeos'])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106s1eIDb2rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_x_and_y(df):\n",
        "    max_x = -1\n",
        "    for x in df['in']:\n",
        "        if(len(x) > max_x):\n",
        "            max_x = len(x)\n",
        "            \n",
        "    max_y = -1\n",
        "    for y in df['out']:\n",
        "        if(len(y) > max_y):\n",
        "            max_y = len(y)\n",
        "    max_y += 1\n",
        "    with open(base_path/'max.txt', 'w+') as f:\n",
        "        f.write(str(max_x) + ',' + str(max_y))\n",
        "    \n",
        "    return max_x, max_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSme7GyGcKnj",
        "colab_type": "code",
        "outputId": "0b08c47d-b946-4d8b-bd9e-38391ee8d9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_max_x_and_y(df)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(370, 683)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO1om97EcNQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_x = None\n",
        "max_y = None\n",
        "with open(base_path/'max.txt') as f:\n",
        "    for line in f:\n",
        "        max_x, max_y = line.split(',')\n",
        "        max_x = int(max_x)\n",
        "        max_y = int(max_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfoUy4ZNwXV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_x_y_tensors(df, vocab, max_x, max_y):\n",
        "    num_samples = len(df.index)\n",
        "    res_x = torch.zeros(num_samples, max_x).long() \n",
        "    res_y = torch.zeros(num_samples, max_y).long()\n",
        "    for sample_idx in range(num_samples):\n",
        "        if(sample_idx % 10000 == 0):\n",
        "            print('done {0}/{1} samples'.format(sample_idx, num_samples))\n",
        "        x, y = df.iloc[sample_idx, :]\n",
        "        \n",
        "        x_tensor = torch.zeros(max_x) + vocab['xxxpad']\n",
        "        y_tensor = torch.zeros(max_y) + vocab['xxxpad']\n",
        "        \n",
        "\n",
        "        num_padding = max_x - len(x)\n",
        "         \n",
        "        #populate the rest of it with actual input\n",
        "        token_index = 0;\n",
        "        for i in range(num_padding, max_x):\n",
        "            x_tensor[i] = vocab[x[token_index]] \n",
        "            token_index += 1\n",
        "               \n",
        "            \n",
        "        #add input to the output\n",
        "        for i in range(len(y)):\n",
        "            y_tensor[i] = vocab[y[i]]\n",
        "            \n",
        "        #add end of stream token\n",
        "        y_tensor[len(y)] = vocab['xxxeos']\n",
        "            \n",
        "        res_x[sample_idx] = x_tensor.long()\n",
        "        res_y[sample_idx] = y_tensor.long()\n",
        "        \n",
        "    return res_x, res_y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lblh9GDmNdy4",
        "colab_type": "code",
        "outputId": "7f0d9e17-373d-4ae6-ee47-b1a51cb35d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "x, y = get_x_y_tensors(df, vocab, max_x, max_y)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 0/221616 samples\n",
            "done 10000/221616 samples\n",
            "done 20000/221616 samples\n",
            "done 30000/221616 samples\n",
            "done 40000/221616 samples\n",
            "done 50000/221616 samples\n",
            "done 60000/221616 samples\n",
            "done 70000/221616 samples\n",
            "done 80000/221616 samples\n",
            "done 90000/221616 samples\n",
            "done 100000/221616 samples\n",
            "done 110000/221616 samples\n",
            "done 120000/221616 samples\n",
            "done 130000/221616 samples\n",
            "done 140000/221616 samples\n",
            "done 150000/221616 samples\n",
            "done 160000/221616 samples\n",
            "done 170000/221616 samples\n",
            "done 180000/221616 samples\n",
            "done 190000/221616 samples\n",
            "done 200000/221616 samples\n",
            "done 210000/221616 samples\n",
            "done 220000/221616 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvkWAx6WfL1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(base_path/'x_tensors.pkl', 'wb') as f:\n",
        "    pickle.dump(x, f)\n",
        "\n",
        "with open(base_path/'y_tensors.pkl', 'wb') as f:\n",
        "    pickle.dump(y, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XtZB2b7fWxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_saved_tensors():\n",
        "    x = None\n",
        "    y = None\n",
        "    with open(base_path/'x_tensors.pkl', 'rb') as f:\n",
        "        x = pickle.load(f)\n",
        "    with open(base_path/'y_tensors.pkl', 'rb') as f:\n",
        "        y = pickle.load(f)\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b66kfmsccrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = get_saved_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f41gwXZEc71G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_data = TensorDataset(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miyd_g3mdFub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, encoder_emb, decoder_emb, num_hidden, output_length, num_layers, padding_idx):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.padding_idx = padding_idx\n",
        "        self.output_length = output_length\n",
        "        \n",
        "        self.encoder_emb_size = encoder_emb.embedding_dim\n",
        "        self.decoder_emb_size = decoder_emb.embedding_dim\n",
        "        self.decoder_vocab_size = decoder_emb.num_embeddings\n",
        "        \n",
        "        self.encoder_emb = encoder_emb\n",
        "        self.encoder_gru = nn.GRU(self.encoder_emb_size, self.num_hidden, num_layers = self.num_layers, batch_first = True)\n",
        "        self.encoder_out = nn.Linear(self.num_hidden, self.decoder_emb_size, bias = False)\n",
        "        \n",
        "        self.decoder_emb = decoder_emb\n",
        "        self.decoder_gru = nn.GRU(self.decoder_emb_size, self.decoder_emb_size, num_layers = self.num_layers, batch_first = True)\n",
        "        self.out = nn.Linear(self.decoder_emb_size, self.decoder_vocab_size)\n",
        "        self.out.weight.data = self.decoder_embedding.weight.data\n",
        "        \n",
        "    def encoder(self, bs, inp):\n",
        "        h = self.init_hidden(bs)\n",
        "        emb = self.encoder_emb(inp)\n",
        "        _, h = self.encoder_gru(emb, h)\n",
        "        h = self.encoder_out(h)\n",
        "        return h\n",
        "    \n",
        "    def decoder(self, decoder_inp, h):\n",
        "        emb = self.decoder_emb(decoder_inp).unsqueeze(1)\n",
        "        out_pred, h = self.decoder_gru(emb, h)\n",
        "        out_pred = self.out(out_pred)\n",
        "        return out_pred, h\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        bs, seq_len = inp.size()\n",
        "        h = self.encoder(bs, inp)\n",
        "        dec_inp = inp.new_zeros(bs).long()\n",
        "        \n",
        "        res = []\n",
        "        for i in range(self.output_length):\n",
        "            h, out_pred = self.decoder(dec_inp, h)\n",
        "            dec_inp = out_pred.max(1)[1]\n",
        "            if (dec_inp==self.pad_idx).all(): break\n",
        "        return torch.stack(res, dim = 1)\n",
        "        \n",
        "    def init_hidden(self, bs): return one_param(self).new_zeros(self.nl, bs, self.nh)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a148UbSuf-Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}